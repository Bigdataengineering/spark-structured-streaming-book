== [[StreamingSymmetricHashJoinExec]] StreamingSymmetricHashJoinExec Binary Physical Operator -- Stream-Stream Joins

`StreamingSymmetricHashJoinExec` is a binary physical operator that represents a `Join` logical operator of two streaming queries at execution time.

[NOTE]
====
A binary physical operator (`BinaryExecNode`) is a physical operator with <<left, left>> and <<right, right>> child physical operators.

Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[BinaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book.
====

[NOTE]
====
`Join` logical operator represents `Dataset.join` operator in a logical query plan.

Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Join.html[Join Logical Operator] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book.
====

`StreamingSymmetricHashJoinExec` requires that the <<joinType, join type>> be `Inner`, `LeftOuter`, or `RightOuter` with the same data types of the <<leftKeys, left>> and the <<rightKeys, right>> keys.

`StreamingSymmetricHashJoinExec` is <<creating-instance, created>> exclusively when <<spark-sql-streaming-StreamingJoinStrategy.adoc#, StreamingJoinStrategy>> execution planning strategy is requested to plan a logical query plan with a `Join` logical operator of two streaming queries with equality predicates (`EqualTo` and `EqualNullSafe`)

[source, scala]
----
// Reduce the number of partitions
// StreamingSymmetricHashJoin physical operator requires:
// 1. The numbers of partitions on left and right are the same
// 2. Uses the numbers of partitions for its own number of partitions
// That makes debugging state stores easier to manage
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1)
assert(spark.sessionState.conf.numShufflePartitions == 1)

val left = spark.readStream.format("rate").load
val right = spark.readStream.format("rate").load
val equiStreamingJoin = left.join(right, "value")
// You cannot use QueryExecution directly
// Since Structured Streaming uses its own "restrictive" QueryExecution
scala> equiStreamingJoin.explain
== Physical Plan ==
*(3) Project [value#190L, timestamp#189, timestamp#193]
+- StreamingSymmetricHashJoin [value#190L], [value#194L], Inner, condition = [ leftOnly = null, rightOnly = null, both = null, full = null ], state info [ checkpoint = <unknown>, runId = 2decf335-c6d4-4810-b95e-abf75181006a, opId = 0, ver = 0, numPartitions = 1], 0, state cleanup [ left = null, right = null ]
   :- Exchange hashpartitioning(value#190L, 1)
   :  +- *(1) Filter isnotnull(value#190L)
   :     +- StreamingRelation rate, [timestamp#189, value#190L]
   +- ReusedExchange [timestamp#193, value#194L], Exchange hashpartitioning(value#190L, 1)

import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val q = equiStreamingJoin
  .writeStream
  .format("console")
  .option("truncate", false)
  .trigger(Trigger.ProcessingTime(30.seconds))
  .queryName("stream-stream-join")
  .start

scala> q.explain
== Physical Plan ==
WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@b807853
+- *(3) Project [value#364L, timestamp#363, timestamp#361]
   +- StreamingSymmetricHashJoin [value#364L], [value#362L], Inner, condition = [ leftOnly = null, rightOnly = null, both = null, full = null ], state info [ checkpoint = file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-376c26ce-c6ae-4572-8e04-927c7a445b46/state, runId = bdfeae3b-3732-4dfe-8d1d-22a089e60fc1, opId = 0, ver = 3, numPartitions = 1], 0, state cleanup [ left = null, right = null ]
      :- Exchange hashpartitioning(value#364L, 1)
      :  +- *(1) Filter isnotnull(value#364L)
      :     +- *(1) Project [timestamp#363, value#364L]
      :        +- *(1) ScanV2 rate[timestamp#363, value#364L]
      +- ReusedExchange [timestamp#361, value#362L], Exchange hashpartitioning(value#364L, 1)

// In the end, stop the query
q.stop
----

`StreamingSymmetricHashJoinExec` is a <<spark-sql-streaming-StateStoreWriter.adoc#, stateful physical operator that writes to a state store>>.

When <<doExecute, executed>>, `StreamingSymmetricHashJoinExec`...FIXME

[[output]]
The output schema of `StreamingSymmetricHashJoinExec` is...FIXME

[[outputPartitioning]]
The output partitioning of `StreamingSymmetricHashJoinExec` is...FIXME

=== [[creating-instance]] Creating StreamingSymmetricHashJoinExec Instance

`StreamingSymmetricHashJoinExec` takes the following to be created:

* [[leftKeys]] Catalyst expressions of the keys on the left side
* [[rightKeys]] Catalyst expressions of the keys on the right side
* [[joinType]] `JoinType`
* [[condition]] Join condition (`JoinConditionSplitPredicates`)
* [[stateInfo]] <<spark-sql-streaming-StatefulOperatorStateInfo.adoc#, StatefulOperatorStateInfo>>
* [[eventTimeWatermark]] Event-time watermark
* [[stateWatermarkPredicates]] State watermark (`JoinStateWatermarkPredicates`)
* [[left]] Physical operator on the left side (`SparkPlan`)
* [[right]] Physical operator on the right side (`SparkPlan`)

`StreamingSymmetricHashJoinExec` initializes the <<internal-properties, internal properties>>.

=== [[metrics]] Performance Metrics

`StreamingSymmetricHashJoinExec` uses the performance metrics of <<spark-sql-streaming-StateStoreWriter.adoc#metrics, StateStoreWriter>>.

=== [[shouldRunAnotherBatch]] `shouldRunAnotherBatch` Method

[source, scala]
----
shouldRunAnotherBatch(newMetadata: OffsetSeqMetadata): Boolean
----

NOTE: `shouldRunAnotherBatch` is part of the <<spark-sql-streaming-StateStoreWriter.adoc#shouldRunAnotherBatch, StateStoreWriter Contract>> to check whether <<spark-sql-streaming-MicroBatchExecution.adoc#, MicroBatchExecution>> should run another batch (based on the updated <<spark-sql-streaming-OffsetSeqMetadata.adoc#, metadata>>).

`shouldRunAnotherBatch`...FIXME

=== [[doExecute]] Executing Physical Operator (Generating Recipe for Distributed Computation as RDD[InternalRow]) -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is part of `SparkPlan` Contract to generate the runtime representation of an physical operator as a recipe for distributed computation over internal binary rows on Apache Spark (`RDD[InternalRow]`).

`doExecute` first requests the `StreamingQueryManager` for the <<spark-sql-streaming-StreamingQueryManager.adoc#stateStoreCoordinator, StateStoreCoordinatorRef>> to the `StateStoreCoordinator` RPC endpoint (for the driver).

`doExecute` then requests the `SymmetricHashJoinStateManager` for the <<spark-sql-streaming-SymmetricHashJoinStateManager.adoc#allStateStoreNames, names of the state stores>> for the left and right side of the streaming join.

In the end, `doExecute` requests the <<left, left>> and <<right, right>> physical operators to execute (generate an RDD) and then <<spark-sql-streaming-StateStoreAwareZipPartitionsHelper.adoc#stateStoreAwareZipPartitions, stateStoreAwareZipPartitions>> with <<processPartitions, processPartitions>> (and with the `StateStoreCoordinatorRef` and the state stores).

=== [[processPartitions]] `processPartitions` Internal Method

[source, scala]
----
processPartitions(
  leftInputIter: Iterator[InternalRow],
  rightInputIter: Iterator[InternalRow]): Iterator[InternalRow]
----

`processPartitions`...FIXME

NOTE: `processPartitions` is used exclusively when `StreamingSymmetricHashJoinExec` physical operator is requested to <<doExecute, execute>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| hadoopConfBcast
a| [[hadoopConfBcast]] Hadoop Configuration broadcast (to the Spark cluster)

Used exclusively to <<joinStateManager, create a SymmetricHashJoinStateManager>>

| joinStateManager
a| [[joinStateManager]] <<spark-sql-streaming-SymmetricHashJoinStateManager.adoc#, SymmetricHashJoinStateManager>>

Used when `OneSideHashJoiner` is requested to <<spark-sql-streaming-StreamingSymmetricHashJoinExec-OneSideHashJoiner.adoc#storeAndJoinWithOtherSide, storeAndJoinWithOtherSide>>, <<spark-sql-streaming-StreamingSymmetricHashJoinExec-OneSideHashJoiner.adoc#removeOldState, removeOldState>>, <<spark-sql-streaming-StreamingSymmetricHashJoinExec-OneSideHashJoiner.adoc#commitStateAndGetMetrics, commitStateAndGetMetrics>>, and for the <<spark-sql-streaming-StreamingSymmetricHashJoinExec-OneSideHashJoiner.adoc#get, values for a given key>>

| nullLeft
a| [[nullLeft]] `GenericInternalRow` of the size of the output schema of the <<left, left physical operator>>

| nullRight
a| [[nullRight]] `GenericInternalRow` of the size of the output schema of the <<right, right physical operator>>

| storeConf
a| [[storeConf]] <<spark-sql-streaming-StateStoreConf.adoc#, StateStoreConf>>

Used exclusively to <<joinStateManager, create a SymmetricHashJoinStateManager>>

|===

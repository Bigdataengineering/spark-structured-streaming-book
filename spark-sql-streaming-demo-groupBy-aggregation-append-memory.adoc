== Demo: Streaming Aggregation with Dataset.groupBy High-Level Operator (Memory Data Source and Append Output Mode)

The following code shows a <<spark-sql-streaming-aggregation.adoc#, streaming aggregation>> (with <<spark-sql-streaming-Dataset-operators.adoc#groupBy, Dataset.groupBy>> operator) in <<spark-sql-streaming-OutputMode.adoc#Append, append>> output mode and <<spark-sql-streaming-MemoryStream.adoc#, MemoryStream>> data source (for the streaming source and the sink).

[source, scala]
----
// START: Only for easier debugging
// The state is then only for one partition
// which should make monitoring easier
val numShufflePartitions = 1
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions)

assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions)
// END: Only for easier debugging

import java.sql.Timestamp
case class Event(time: Timestamp, value: Long)
import scala.concurrent.duration._
object Event {
  def apply(n: Long, value: Long): Event = {
    Event(new Timestamp(n.seconds.toMillis), value)
  }
}

import org.apache.spark.sql.execution.streaming.MemoryStream
implicit val sqlCtx = spark.sqlContext
val events = MemoryStream[Event]
events.addData(
  Event(1,1),
  Event(1,2),
  Event(3,3),
  Event(4,3),
  Event(5,2),
  Event(6,2),
  Event(10,9))
val values = events.toDS

assert(values.isStreaming, "values must be a streaming Dataset")

import org.apache.spark.sql.functions.window
val countsPer5secWindow = values
  .withWatermark(eventTime = "time", delayThreshold = "0 seconds")
  .groupBy(window($"time", "5 seconds") as "window", $"value")
  .agg(count("*") as "count") // <-- creates an Aggregate logical operator

scala> countsPer5secWindow.printSchema
root
 |-- window: struct (nullable = false)
 |    |-- start: timestamp (nullable = true)
 |    |-- end: timestamp (nullable = true)
 |-- value: long (nullable = false)
 |-- count: long (nullable = false)

val queryName = "counts"
val checkpointLocation = s"/tmp/checkpoint-$queryName"

// Delete the checkpoint location from previous executions
import java.nio.file.{Files, FileSystems}
import java.util.Comparator
import scala.collection.JavaConverters._
val path = FileSystems.getDefault.getPath(checkpointLocation)
if (Files.exists(path)) {
  Files.walk(path)
    .sorted(Comparator.reverseOrder())
    .iterator
    .asScala
    .foreach(p => p.toFile.delete)
}

import org.apache.spark.sql.streaming.OutputMode.Append
val streamingQuery = countsPer5secWindow
  .writeStream
  .format("memory")
  .queryName(queryName)
  .option("checkpointLocation", checkpointLocation)
  .outputMode(Append)
  .start

scala> streamingQuery.explain
== Physical Plan ==
*(4) HashAggregate(keys=[window#138-T0ms, value#128L], functions=[count(1)])
+- StateStoreSave [window#138-T0ms, value#128L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 52ee0fdf-b64d-4954-84ac-1bb4597861b2, opId = 0, ver = 1, numPartitions = 1], Append, 10000, 2
   +- *(3) HashAggregate(keys=[window#138-T0ms, value#128L], functions=[merge_count(1)])
      +- StateStoreRestore [window#138-T0ms, value#128L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 52ee0fdf-b64d-4954-84ac-1bb4597861b2, opId = 0, ver = 1, numPartitions = 1], 2
         +- *(2) HashAggregate(keys=[window#138-T0ms, value#128L], functions=[merge_count(1)])
            +- Exchange hashpartitioning(window#138-T0ms, value#128L, 1)
               +- *(1) HashAggregate(keys=[window#138-T0ms, value#128L], functions=[partial_count(1)])
                  +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#127-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#127-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#127-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#127-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#127-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#127-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#127-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#127-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#138-T0ms, value#128L]
                     +- *(1) Filter isnotnull(time#127-T0ms)
                        +- EventTimeWatermark time#127: timestamp, interval
                           +- LocalTableScan <empty>, [time#127, value#128L]

streamingQuery.processAllAvailable()

// Use web UI to monitor the state of state (no pun intended)
// StateStoreSave and StateStoreRestore operators all have state metrics
// Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs

// You may also want to check out checkpointed state
// in /tmp/checkpoint-counts/state/0/0

val output = spark.table(queryName).orderBy("window", "value")
output.show(truncate = false)
/**
+------------------------------------------+-----+-----+
|window                                    |value|count|
+------------------------------------------+-----+-----+
|[1970-01-01 01:00:00, 1970-01-01 01:00:05]|1    |1    |
|[1970-01-01 01:00:00, 1970-01-01 01:00:05]|2    |1    |
|[1970-01-01 01:00:00, 1970-01-01 01:00:05]|3    |2    |
|[1970-01-01 01:00:05, 1970-01-01 01:00:10]|2    |2    |
+------------------------------------------+-----+-----+
*/
assert(output.collect.size == 4)

// Eventually...
streamingQuery.stop()
----

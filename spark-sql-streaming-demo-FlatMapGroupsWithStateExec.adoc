== Demo: The Internals of FlatMapGroupsWithStateExec Physical Operator

The following demo shows the internals of <<spark-sql-streaming-FlatMapGroupsWithStateExec.adoc#, FlatMapGroupsWithStateExec>> physical operator in a <<spark-sql-streaming-stateful-stream-processing.adoc#, stateful streaming query>>.

[source, scala]
----
// Reduce the number of partitions and hence the state stores
// That is supposed to make debugging state checkpointing easier
val numShufflePartitions = 1
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions)

assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions)

// Define event "format"
// Use :paste mode in spark-shell
import java.sql.Timestamp
case class Event(time: Timestamp, value: Long)
import scala.concurrent.duration._
object Event {
  def apply(secs: Long, value: Long): Event = {
    Event(new Timestamp(secs.seconds.toMillis), value)
  }
}

// Using memory data source for full control of the input
import org.apache.spark.sql.execution.streaming.MemoryStream
implicit val sqlCtx = spark.sqlContext
val events = MemoryStream[Event]
val values = events.toDS
assert(values.isStreaming, "values must be a streaming Dataset")

values.printSchema
/**
scala> values.printSchema
root
 |-- time: timestamp (nullable = true)
 |-- value: long (nullable = false)
*/

import scala.concurrent.duration._
val delayThreshold = 10.seconds
val valuesWatermarked = values
  .withWatermark(eventTime = "time", delayThreshold.toString) // required for EventTimeTimeout

import java.sql.Timestamp
import org.apache.spark.sql.streaming.GroupState
val stateFn = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Long]) => {
  Iterator((key, values.size))
}

import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode}
val valuesGrouped = valuesWatermarked
  .as[(Timestamp, Long)] // convert DataFrame to Dataset to make groupByKey easier to write
  .groupByKey { case (time, value) => value % 2 }
  .flatMapGroupsWithState(
    OutputMode.Update, timeoutConf = GroupStateTimeout.EventTimeTimeout)(stateFn)

valuesGrouped.explain
/**
== Physical Plan ==
*(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#27L, assertnotnull(input[0, scala.Tuple2, true])._2 AS _2#28]
+- FlatMapGroupsWithState $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$2322/1350789239@6be78a46, value#22: bigint, newInstance(class scala.Tuple2), [value#22L], [time#2-T10000ms, value#3L], obj#26: scala.Tuple2, state info [ checkpoint = <unknown>, runId = adeb83b1-4281-4c16-9447-24dc100181f9, opId = 0, ver = 0, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 0, 0
   +- *(1) Sort [value#22L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(value#22L, 1)
         +- AppendColumns $line23.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$2336/890123495@16abeca6, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#22L]
            +- EventTimeWatermark time#2: timestamp, interval 10 seconds
               +- StreamingRelation MemoryStream[time#2,value#3L], [time#2, value#3L]
*/

val queryName = "FlatMapGroupsWithStateExec_demo"
val checkpointLocation = s"/tmp/checkpoint-$queryName"

// Delete the checkpoint location from previous executions
import java.nio.file.{Files, FileSystems}
import java.util.Comparator
import scala.collection.JavaConverters._
val path = FileSystems.getDefault.getPath(checkpointLocation)
if (Files.exists(path)) {
  Files.walk(path)
    .sorted(Comparator.reverseOrder())
    .iterator
    .asScala
    .foreach(p => p.toFile.delete)
}

import org.apache.spark.sql.streaming.OutputMode.Update
val streamingQuery = valuesGrouped
  .writeStream
  .format("memory")
  .queryName(queryName)
  .option("checkpointLocation", checkpointLocation)
  .outputMode(Update)
  .start

assert(streamingQuery.status.message == "Waiting for data to arrive")

// Use web UI to monitor the metrics of the streaming query
// Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs

// You may also want to check out checkpointed state
// in /tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state/0/0

val batch = Seq(
  Event(1,  1),
  Event(15, 2))
events.addData(batch)
streamingQuery.processAllAvailable()

spark.table(queryName).show(truncate = false)
/**
+---+---+
|_1 |_2 |
+---+---+
|0  |1  |
|1  |1  |
+---+---+
*/

// With at least one execution we can review the execution plan
streamingQuery.explain
/**
== Physical Plan ==
*(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#27L, assertnotnull(input[0, scala.Tuple2, true])._2 AS _2#28]
+- FlatMapGroupsWithState $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$2322/1350789239@6be78a46, value#22: bigint, newInstance(class scala.Tuple2), [value#22L], [time#2-T10000ms, value#3L], obj#26: scala.Tuple2, state info [ checkpoint = file:/tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state, runId = f8cbc1cb-0c91-4780-bab6-a2de4e361975, opId = 0, ver = 1, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 1561722000476, 5000
   +- *(1) Sort [value#22L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(value#22L, 1)
         +- AppendColumns $line23.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$2336/890123495@16abeca6, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#22L]
            +- EventTimeWatermark time#2: timestamp, interval 10 seconds
               +- LocalTableScan <empty>, [time#2, value#3L]
*/

// Let's access the FlatMapGroupsWithStateExec physical operator
import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper
import org.apache.spark.sql.execution.streaming.StreamExecution
val engine: StreamExecution = streamingQuery
  .asInstanceOf[StreamingQueryWrapper]
  .streamingQuery

import org.apache.spark.sql.execution.streaming.IncrementalExecution
val lastMicroBatch: IncrementalExecution = engine.lastExecution

// Access executedPlan that is the optimized physical query plan ready for execution
// All streaming optimizations have been applied at this point
// We just need the FlatMapGroupsWithStateExec physical operator
import org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec
val flatMapOp = plan.collect { case op: FlatMapGroupsWithStateExec => op }.head

// Display metrics
import org.apache.spark.sql.execution.metric.SQLMetric
def formatMetrics(name: String, metric: SQLMetric) = {
  // name, desc, value
  val desc = metric.name.getOrElse("")
  val value = metric.value
  s"$name, $desc, $value"
}
flatMapOp.metrics.map { case (name, metric) => formatMetrics(name, metric) }.foreach(println)
/**
numTotalStateRows, number of total state rows, 0
stateMemory, memory used by state total (min, med, max), 390
loadedMapCacheHitCount, count of cache hit on states cache in provider, 1
numOutputRows, number of output rows, 0
stateOnCurrentVersionSizeBytes, estimated size of state only on current version total (min, med, max), 102
loadedMapCacheMissCount, count of cache miss on states cache in provider, 0
commitTimeMs, time to commit changes total (min, med, max), -2
allRemovalsTimeMs, total time to remove rows total (min, med, max), -2
numUpdatedStateRows, number of updated state rows, 0
allUpdatesTimeMs, total time to update rows total (min, med, max), -2
*/

// Eventually...
streamingQuery.stop()
----

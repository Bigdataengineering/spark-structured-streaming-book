== [[HDFSBackedStateStoreProvider]] HDFSBackedStateStoreProvider -- Hadoop DFS-based StateStoreProvider

`HDFSBackedStateStoreProvider` is a <<spark-sql-streaming-StateStoreProvider.adoc#, StateStoreProvider>> that uses Hadoop DFS for <<baseDir, state checkpointing>>.

`HDFSBackedStateStoreProvider` is the default `StateStoreProvider` per the <<spark-sql-streaming-properties.adoc#spark.sql.streaming.stateStore.providerClass, spark.sql.streaming.stateStore.providerClass>> internal configuration property.

`HDFSBackedStateStoreProvider` is <<creating-instance, created>> and immediately requested to <<init, initialize>> when `StateStoreProvider` utility is requested to <<spark-sql-streaming-StateStoreProvider.adoc#createAndInit, create and initialize a StateStoreProvider>>. That is when `HDFSBackedStateStoreProvider` is given the <<stateStoreId, StateStoreId>> that uniquely identifies the <<spark-sql-streaming-StateStore.adoc#, state store>> to use for a stateful operator and a partition.

[[creating-instance]]
`HDFSBackedStateStoreProvider` takes no arguments to be created.

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider=ALL
```

Refer to <<spark-sql-streaming-logging.adoc#, Logging>>.
====

=== [[baseDir]] State Checkpoint Base Directory -- `baseDir` Lazy Internal Property

[source,scala]
----
baseDir: Path
----

`baseDir` is the base directory (as Hadoop DFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path]) for <<spark-sql-streaming-offsets-and-metadata-checkpointing.adoc#, state checkpointing>> (for <<deltaFile, delta>> and <<snapshotFile, snapshot>> state files).

`baseDir` is initialized lazily since it is not yet known when `HDFSBackedStateStoreProvider` is <<creating-instance, created>>.

`baseDir` is initialized and created based on the <<spark-sql-streaming-StateStoreId.adoc#storeCheckpointLocation, state checkpoint base directory>> of the <<stateStoreId, StateStoreId>> when `HDFSBackedStateStoreProvider` is requested to <<init, initialize>>.

=== [[stateStoreId]][[stateStoreId_]] StateStoreId -- Unique Identifier of State Store

As a <<spark-sql-streaming-StateStoreProvider.adoc#, StateStoreProvider>>, `HDFSBackedStateStoreProvider` is associated with a <<spark-sql-streaming-StateStoreProvider.adoc#stateStoreId, StateStoreId>> (which is a unique identifier of the <<spark-sql-streaming-StateStore.adoc#, state store>> for a stateful operator and a partition).

`HDFSBackedStateStoreProvider` is given the <<stateStoreId, StateStoreId>> at <<init, initialization>> (as requested by the <<spark-sql-streaming-StateStoreProvider.adoc#, StateStoreProvider>> contract).

The <<stateStoreId, StateStoreId>> is then used for the following:

* `HDFSBackedStateStore` is requested for the <<spark-sql-streaming-HDFSBackedStateStore.adoc#id, id>>

* `HDFSBackedStateStoreProvider` is requested for the <<toString, textual representation>> and the <<baseDir, state checkpoint base directory>>

=== [[toString]] Textual Representation -- `toString` Method

[source, scala]
----
toString(): String
----

NOTE: `toString` is part of the link:++https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Object.html#toString()++[java.lang.Object] contract for the string representation of the object.

`HDFSBackedStateStoreProvider` uses the <<stateStoreId, StateStoreId>> and the <<baseDir, state checkpoint base directory>> for the textual representation:

```
HDFSStateStoreProvider[id = (op=[operatorId],part=[partitionId]),dir = [baseDir]]
```

=== [[getStore]] Retrieving State Store by Version -- `getStore` Method

[source, scala]
----
getStore(version: Long): StateStore
----

NOTE: `getStore` is part of the <<spark-sql-streaming-StateStoreProvider.adoc#getStore, StateStoreProvider Contract>> to get the <<spark-sql-streaming-StateStore.adoc#, StateStore>> for a given version.

`getStore`...FIXME

=== [[deltaFile]] `deltaFile` Internal Method

[source, scala]
----
deltaFile(version: Long): Path
----

`deltaFile` simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] of the `[version].delta` file in the <<baseDir, state checkpoint base directory>>.

[NOTE]
====
`deltaFile` is used when:

* <<spark-sql-streaming-HDFSBackedStateStore.adoc#, HDFSBackedStateStore>> is created (and creates the <<finalDeltaFile, final delta file>>)

* `HDFSBackedStateStoreProvider` is requested to <<updateFromDeltaFile, updateFromDeltaFile>>
====

=== [[fetchFiles]] Fetching All Delta And Snapshot Files -- `fetchFiles` Internal Method

[source, scala]
----
fetchFiles(): Seq[StoreFile]
----

`fetchFiles`...FIXME

NOTE: `fetchFiles` is used when `HDFSBackedStateStoreProvider` is requested to <<latestIterator, latestIterator>>, <<doSnapshot, doSnapshot>> and <<cleanup, cleanup>>.

=== [[init]] Initializing StateStoreProvider -- `init` Method

[source, scala]
----
init(
  stateStoreId: StateStoreId,
  keySchema: StructType,
  valueSchema: StructType,
  indexOrdinal: Option[Int],
  storeConf: StateStoreConf,
  hadoopConf: Configuration): Unit
----

NOTE: `init` is part of the <<spark-sql-streaming-StateStoreProvider.adoc#init, StateStoreProvider Contract>> to initialize itself.

`init` records the values of the input arguments as the <<stateStoreId, stateStoreId>>, <<keySchema, keySchema>>, <<valueSchema, valueSchema>>, <<storeConf, storeConf>>, and <<hadoopConf, hadoopConf>> internal properties.

`init` requests the given `StateStoreConf` for the <<spark-sql-streaming-StateStoreConf.adoc#maxVersionsToRetainInMemory, spark.sql.streaming.maxBatchesToRetainInMemory>> configuration property (that is then recorded in the <<numberOfVersionsToRetainInMemory, numberOfVersionsToRetainInMemory>> internal property).

In the end, `init` requests the <<fm, CheckpointFileManager>> to <<spark-sql-streaming-CheckpointFileManager.adoc#mkdirs, create>> the <<baseDir, baseDir>> directory (with parent directories).

=== [[latestIterator]] `latestIterator` Internal Method

[source, scala]
----
latestIterator(): Iterator[UnsafeRowPair]
----

`latestIterator`...FIXME

NOTE: `latestIterator` seems to be used exclusively in tests.

=== [[doSnapshot]] `doSnapshot` Internal Method

[source, scala]
----
doSnapshot(): Unit
----

`doSnapshot`...FIXME

NOTE: `doSnapshot` is used when...FIXME

=== [[cleanup]] Cleaning Up -- `cleanup` Internal Method

[source, scala]
----
cleanup(): Unit
----

`cleanup`...FIXME

NOTE: `cleanup` is used exclusively when <<doMaintenance, doMaintenance>>.

=== [[doMaintenance]] Doing Maintenance -- `doMaintenance` Method

[source, scala]
----
doMaintenance(): Unit
----

NOTE: `doMaintenance` is part of the <<spark-sql-streaming-StateStoreProvider.adoc#doMaintenance, StateStoreProvider Contract>> to do maintenance if needed.

`doMaintenance`...FIXME

=== [[close]] Closing State Store Provider -- `close` Method

[source, scala]
----
close(): Unit
----

NOTE: `close` is part of the <<spark-sql-streaming-StateStoreProvider.adoc#close, StateStoreProvider Contract>> to close the state store provider.

`close`...FIXME

=== [[commitUpdates]] Committing State Changes (As New Version of State) -- `commitUpdates` Internal Method

[source, scala]
----
commitUpdates(
  newVersion: Long,
  map: ConcurrentHashMap[UnsafeRow, UnsafeRow],
  output: DataOutputStream): Unit
----

`commitUpdates` <<finalizeDeltaFile, finalizeDeltaFile>> (with the given `DataOutputStream`) followed by <<putStateIntoStateCacheMap, caching the new version of state>> (with the given `newVersion` and the `map` state).

NOTE: `commitUpdates` is used exclusively when `HDFSBackedStateStore` is requested to <<spark-sql-streaming-HDFSBackedStateStore.adoc#commit, commit state changes>>.

=== [[loadMap]] Loading State For Specified Version -- `loadMap` Internal Method

[source, scala]
----
loadMap(
  version: Long): ConcurrentHashMap[UnsafeRow, UnsafeRow]
----

`loadMap`...FIXME

NOTE: `loadMap` is used when `HDFSBackedStateStoreProvider` is requested to <<getStore, retrieve the state store for a specified version>> and <<latestIterator, latestIterator>>.

=== [[putStateIntoStateCacheMap]] Caching New Version of State -- `putStateIntoStateCacheMap` Internal Method

[source, scala]
----
putStateIntoStateCacheMap(
  newVersion: Long,
  map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit
----

`putStateIntoStateCacheMap` registers state for a given version, i.e. adds the `map` state under the `newVersion` key in the <<loadedMaps, loadedMaps>> internal registry.

With the <<numberOfVersionsToRetainInMemory, numberOfVersionsToRetainInMemory>> threshold as `0` or below, `putStateIntoStateCacheMap` simply removes all entries from the <<loadedMaps, loadedMaps>> internal registry and returns.

`putStateIntoStateCacheMap` removes the oldest state version(s) in the <<loadedMaps, loadedMaps>> internal registry until its size is at the <<numberOfVersionsToRetainInMemory, numberOfVersionsToRetainInMemory>> threshold.

With the size of the <<loadedMaps, loadedMaps>> internal registry is at the <<numberOfVersionsToRetainInMemory, numberOfVersionsToRetainInMemory>> threshold, `putStateIntoStateCacheMap` does two more optimizations per `newVersion`

* It does not add the given state when the version of the oldest state is earlier (larger) than the given `newVersion`

* It removes the oldest state when older (smaller) than the given `newVersion`

NOTE: `putStateIntoStateCacheMap` is used when `HDFSBackedStateStoreProvider` is requested to <<commitUpdates, commit state (as a new version)>> and <<loadMap, load state for a specified version>>.

=== [[writeSnapshotFile]] `writeSnapshotFile` Internal Method

[source, scala]
----
writeSnapshotFile(
  version: Long,
  map: MapType): Unit
----

`writeSnapshotFile`...FIXME

NOTE: `writeSnapshotFile` is used when...FIXME

=== [[updateFromDeltaFile]] `updateFromDeltaFile` Internal Method

[source, scala]
----
updateFromDeltaFile(
  version: Long,
  map: MapType): Unit
----

`updateFromDeltaFile`...FIXME

NOTE: `updateFromDeltaFile` is used exclusively when `HDFSBackedStateStoreProvider` is requested to <<loadMap, loadMap>>.

=== [[readSnapshotFile]] `readSnapshotFile` Internal Method

[source, scala]
----
readSnapshotFile(
  version: Long): Option[MapType]
----

`readSnapshotFile`...FIXME

NOTE: `readSnapshotFile` is used...FIXME

=== [[finalizeDeltaFile]] `finalizeDeltaFile` Internal Method

[source, scala]
----
finalizeDeltaFile(
  output: DataOutputStream): Unit
----

`finalizeDeltaFile` simply writes `-1` to the given `DataOutputStream` (to indicate end of file) and closes it.

NOTE: `finalizeDeltaFile` is used exclusively when `HDFSBackedStateStoreProvider` is requested to <<commitUpdates, commit state changes (a new version of state)>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| fm
a| [[fm]] <<spark-sql-streaming-CheckpointFileManager.adoc#, CheckpointFileManager>>

| loadedMaps
a| [[loadedMaps]]

[source, scala]
----
loadedMaps: TreeMap[
  Long,                                     // state version
  ConcurrentHashMap[UnsafeRow, UnsafeRow]]  // state (as keys and values)
----

https://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html[java.util.TreeMap] of state versions sorted according to the reversed ordering of the versions (i.e. long numbers)

The current size estimation of `loadedMaps` is the <<memoryUsedBytes, memoryUsedBytes>> metric in the <<metrics, metrics>>.

A new entry (a version and the associated map) is added exclusively when `HDFSBackedStateStoreProvider` is requested to <<putStateIntoStateCacheMap, putStateIntoStateCacheMap>>.

Used when...FIXME

| numberOfVersionsToRetainInMemory
a| [[numberOfVersionsToRetainInMemory]]

[source, scala]
----
numberOfVersionsToRetainInMemory: Int
----

`numberOfVersionsToRetainInMemory` is the maximum number of entries in the <<loadedMaps, loadedMaps>> internal registry and is configured by the <<spark-sql-streaming-properties.adoc#spark.sql.streaming.maxBatchesToRetainInMemory, spark.sql.streaming.maxBatchesToRetainInMemory>> internal configuration.

`numberOfVersionsToRetainInMemory` is a threshold when `HDFSBackedStateStoreProvider` removes the last key from the <<loadedMaps, loadedMaps>> internal registry (per reverse ordering of state versions) when requested to <<putStateIntoStateCacheMap, putStateIntoStateCacheMap>>.
|===

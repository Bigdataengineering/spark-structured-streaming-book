== [[StateStoreRestoreExec]] StateStoreRestoreExec Unary Physical Operator -- Restoring State of Streaming Aggregates

`StateStoreRestoreExec` is a unary physical operator that <<spark-sql-streaming-StateStoreReader.adoc#, reads (restores) a streaming state from a state store>> (for the keys from the <<child, child>> physical operator).

[NOTE]
====
A unary physical operator (`UnaryExecNode`) is a physical operator with a single <<child, child>> physical operator.

Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book.
====

`StateStoreRestoreExec` is <<creating-instance, created>> exclusively when <<spark-sql-streaming-StatefulAggregationStrategy.adoc#, StatefulAggregationStrategy>> execution planning strategy is requested to plan a <<spark-sql-streaming-aggregation.adoc#, streaming aggregation>> for execution (`Aggregate` logical operators in the logical plan of a streaming query).

.StateStoreRestoreExec and StatefulAggregationStrategy
image::images/StateStoreRestoreExec-StatefulAggregationStrategy.png[align="center"]

The optional <<stateInfo, StatefulOperatorStateInfo>> is initially undefined (i.e. when `StateStoreRestoreExec` is <<creating-instance, created>>). `StateStoreRestoreExec` is updated to hold the streaming batch-specific execution property when `IncrementalExecution` link:spark-sql-streaming-IncrementalExecution.adoc#preparations[prepares a streaming physical plan for execution] (and link:spark-sql-streaming-IncrementalExecution.adoc#state[state] preparation rule is executed when `StreamExecution` link:spark-sql-streaming-MicroBatchExecution.adoc#runBatch-queryPlanning[plans a streaming query] for a streaming batch).

.StateStoreRestoreExec and IncrementalExecution
image::images/StateStoreRestoreExec-IncrementalExecution.png[align="center"]

When <<doExecute, executed>>, `StateStoreRestoreExec` executes the <<child, child>> physical operator and <<spark-sql-streaming-StateStoreOps.adoc#mapPartitionsWithStateStore, creates a StateStoreRDD to map over partitions>> with `storeUpdateFunction` that restores the state for the keys in the input rows if available.

[[output]]
The output schema of `StateStoreRestoreExec` is exactly the <<child, child>>'s output schema.

[[outputPartitioning]]
The output partitioning of `StateStoreRestoreExec` is exactly the <<child, child>>'s output partitioning.

=== [[demo]] Demo: StateStoreRestoreExec Operator in Physical Plan of Streaming Aggregation (Dataset.groupBy Operator)

[source, scala]
----
val query = spark.
  readStream.
  format("rate").
  load.
  withWatermark(eventTime = "timestamp", delayThreshold = "20 seconds").
  groupBy(window($"timestamp", "5 seconds") as "group").
  agg(count("value") as "value_count").
  orderBy($"value_count".asc)

// Logical plan with Aggregate logical operator
val logicalPlan = query.queryExecution.logical
scala> println(logicalPlan.numberedTreeString)
00 'Sort ['value_count ASC NULLS FIRST], true
01 +- Aggregate [window#25-T20000ms], [window#25-T20000ms AS group#19, count(value#15L) AS value_count#24L]
02    +- Filter isnotnull(timestamp#14-T20000ms)
03       +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / cast(5000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / cast(5000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / cast(5000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / cast(5000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / cast(5000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / cast(5000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / cast(5000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / cast(5000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 5000000) + 0) + 5000000), LongType, TimestampType)) AS window#25-T20000ms, timestamp#14-T20000ms, value#15L]
04          +- EventTimeWatermark timestamp#14: timestamp, interval 20 seconds
05             +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@6364e814, rate, [timestamp#14, value#15L]

// Physical plan with StateStoreRestoreExec (as StateStoreRestore in the output)
scala> query.explain
== Physical Plan ==
*(5) Sort [value_count#24L ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(value_count#24L ASC NULLS FIRST, 200)
   +- *(4) HashAggregate(keys=[window#25-T20000ms], functions=[count(value#15L)])
      +- StateStoreSave [window#25-T20000ms], state info [ checkpoint = <unknown>, runId = 13c39b49-2fda-4575-bffb-08e94614420a, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2
         +- *(3) HashAggregate(keys=[window#25-T20000ms], functions=[merge_count(value#15L)])
            +- StateStoreRestore [window#25-T20000ms], state info [ checkpoint = <unknown>, runId = 13c39b49-2fda-4575-bffb-08e94614420a, opId = 0, ver = 0, numPartitions = 200], 2
               +- *(2) HashAggregate(keys=[window#25-T20000ms], functions=[merge_count(value#15L)])
                  +- Exchange hashpartitioning(window#25-T20000ms, 200)
                     +- *(1) HashAggregate(keys=[window#25-T20000ms], functions=[partial_count(value#15L)])
                        +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#14-T20000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#25-T20000ms, value#15L]
                           +- *(1) Filter isnotnull(timestamp#14-T20000ms)
                              +- EventTimeWatermark timestamp#14: timestamp, interval 20 seconds
                                 +- StreamingRelation rate, [timestamp#14, value#15L]
----

=== [[metrics]] Performance Metrics (SQLMetrics)

[cols="1m,1,3",options="header",width="100%"]
|===
| Key
| Name (in UI)
| Description

| numOutputRows
| number of output rows
| [[numOutputRows]] The number of input rows from the <<child, child>> physical operator (for which `StateStoreRestoreExec` tried to find the state)
|===

.StateStoreRestoreExec in web UI (Details for Query)
image::images/StateStoreRestoreExec-webui-query-details.png[align="center"]

=== [[creating-instance]] Creating StateStoreRestoreExec Instance

`StateStoreRestoreExec` takes the following to be created:

* [[keyExpressions]] *Key expressions*, i.e. Catalyst attributes for the grouping keys
* [[stateInfo]] Optional <<spark-sql-streaming-StatefulOperatorStateInfo.adoc#, StatefulOperatorStateInfo>> (default: `None`)
* [[stateFormatVersion]] Version of the state format (based on the <<spark-sql-streaming-properties.adoc#spark.sql.streaming.aggregation.stateFormatVersion, spark.sql.streaming.aggregation.stateFormatVersion>> configuration property)
* [[child]] Child physical operator (`SparkPlan`)

=== [[stateManager]] StateStoreRestoreExec and StreamingAggregationStateManager -- `stateManager` Property

[source, scala]
----
stateManager: StreamingAggregationStateManager
----

`stateManager` is a <<spark-sql-streaming-StreamingAggregationStateManager.adoc#, StreamingAggregationStateManager>> that is created together with `StateStoreRestoreExec`.

The `StreamingAggregationStateManager` is created for the <<keyExpressions, keys>>, the output schema of the <<child, child>> physical operator and the <<stateFormatVersion, version of the state format>>.

The `StreamingAggregationStateManager` is used when `StateStoreRestoreExec` is requested to <<doExecute, generate a recipe for a distributed computation (as a RDD[InternalRow])>> for the following:

* <<spark-sql-streaming-StreamingAggregationStateManager.adoc#getStateValueSchema, Schema of the values in a state store>>

* <<spark-sql-streaming-StreamingAggregationStateManager.adoc#getKey, Extracting the columns for the key from the input row>>

* <<spark-sql-streaming-StreamingAggregationStateManager.adoc#get, Looking up the value of a key from a state store>>

=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is part of `SparkPlan` Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. `RDD[InternalRow]`).

Internally, `doExecute` executes <<child, child>> physical operator and link:spark-sql-streaming-StateStoreOps.adoc#mapPartitionsWithStateStore[creates a StateStoreRDD] with `storeUpdateFunction` that does the following per <<child, child>> operator's RDD partition:

1. Generates an unsafe projection to access the key field (using <<keyExpressions, keyExpressions>> and the output schema of <<child, child>> operator).

1. For every input row (as `InternalRow`)

* Extracts the key from the row (using the unsafe projection above)

* link:spark-sql-streaming-StateStore.adoc#get[Gets the saved state] in `StateStore` for the key if available (it might not be if the key appeared in the input the first time)

* Increments <<numOutputRows, numOutputRows>> metric (that in the end is the number of rows from the <<child, child>> operator)

* Generates collection made up of the current row and possibly the state for the key if available

NOTE: The number of rows from `StateStoreRestoreExec` is the number of rows from the <<child, child>> operator with additional rows for the saved state.

NOTE: There is no way in `StateStoreRestoreExec` to find out how many rows had associated state available in a state store. You would have to use the corresponding `StateStoreSaveExec` operator's link:spark-sql-streaming-StateStoreSaveExec.adoc#metrics[metrics] (most likely `number of total state rows` but that could depend on the output mode).

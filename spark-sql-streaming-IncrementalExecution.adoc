== [[IncrementalExecution]] IncrementalExecution -- QueryExecution of Streaming Queries

`IncrementalExecution` is the `QueryExecution` of streaming queries.

TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-QueryExecution.html[QueryExecution] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book.

`IncrementalExecution` is used when <<spark-sql-streaming-MicroBatchExecution.adoc#, StreamExecution>> is requested to <<spark-sql-streaming-MicroBatchExecution.adoc#runBatch-queryPlanning, incrementally execute>> the <<logicalPlan, logical query plan>> every trigger.

.StreamExecution creates IncrementalExecution (every trigger / streaming batch)
image::images/IncrementalExecution-StreamExecution.png[align="center"]

[[preparations]]
`IncrementalExecution` registers the <<state, state>> preparation rule with the parent ``QueryExecution``'s `preparations` that prepares the streaming physical plan (using batch-specific execution properties).

`IncrementalExecution` is <<creating-instance, created>> when:

* `StreamExecution` link:spark-sql-streaming-MicroBatchExecution.adoc#runBatch-queryPlanning[plans a streaming query]

* `ExplainCommand` is executed (for link:spark-sql-streaming-Dataset-operators.adoc#explain[explain] operator)

`IncrementalExecution` uses the <<checkpointLocation, state checkpoint directory>> (that is given when `IncrementalExecution` is <<creating-instance, created>>) that is one of the following:

* *<unknown>* when `explain` command is executed (on a streaming query)

* *state* directory under <<spark-sql-streaming-StreamExecution.adoc#resolvedCheckpointRoot, checkpoint directory>> (i.e. <<spark-sql-streaming-DataStreamWriter.adoc#checkpointLocation, checkpointLocation>> option or <<spark-sql-streaming-properties.adoc#spark.sql.streaming.checkpointLocation, spark.sql.streaming.checkpointLocation>> configuration property with `queryName` option added)

.Demo: State Checkpoint Directory
[source, scala]
----
// START: Only for easier debugging
// The state is then only for one partition
// which should make monitoring easier
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1)

assert(spark.sessionState.conf.numShufflePartitions == 1)
// END: Only for easier debugging

val counts = spark
  .readStream
  .format("rate")
  .load
  .groupBy(window($"timestamp", "5 seconds") as "group")
  .agg(count("value") as "value_count") // <-- creates an Aggregate logical operator
  .orderBy("group")  // <-- makes for easier checking

assert(counts.isStreaming, "This should be a streaming query")

// Search for "checkpoint = <unknown>" in the following output
// Looks for StateStoreSave and StateStoreRestore
scala> counts.explain
== Physical Plan ==
*(5) Sort [group#5 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(group#5 ASC NULLS FIRST, 1)
   +- *(4) HashAggregate(keys=[window#11], functions=[count(value#1L)])
      +- StateStoreSave [window#11], state info [ checkpoint = <unknown>, runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2
         +- *(3) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)])
            +- StateStoreRestore [window#11], state info [ checkpoint = <unknown>, runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], 2
               +- *(2) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)])
                  +- Exchange hashpartitioning(window#11, 1)
                     +- *(1) HashAggregate(keys=[window#11], functions=[partial_count(value#1L)])
                        +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#11, value#1L]
                           +- *(1) Filter isnotnull(timestamp#0)
                              +- StreamingRelation rate, [timestamp#0, value#1L]

// Start the query to access lastExecution that has the checkpoint resolved
import scala.concurrent.duration._
import org.apache.spark.sql.streaming.{OutputMode, Trigger}
val t = Trigger.ProcessingTime(1.hour) // should be enough time for exploration
val sq = counts
  .writeStream
  .format("console")
  .option("truncate", false)
  .option("checkpointLocation", "/tmp/spark-streams-state-checkpoint-root")
  .trigger(t)
  .outputMode(OutputMode.Complete)
  .start

// wait till the first batch which should happen right after start

import org.apache.spark.sql.execution.streaming._
val lastExecution = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery.lastExecution
scala> println(lastExecution.checkpointLocation)
file:/tmp/spark-streams-state-checkpoint-root/state
----

[[internal-registries]]
.IncrementalExecution's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[planner]] `planner`
a| `SparkPlanner` with the following extra planning strategies (in the order of execution):

[[extraPlanningStrategies]]
* <<spark-sql-streaming-StreamingJoinStrategy.adoc#, StreamingJoinStrategy>>
* <<spark-sql-streaming-StatefulAggregationStrategy.adoc#, StatefulAggregationStrategy>>
* <<spark-sql-streaming-FlatMapGroupsWithStateStrategy.adoc#, FlatMapGroupsWithStateStrategy>>
* <<spark-sql-streaming-StreamingRelationStrategy.adoc#, StreamingRelationStrategy>>
* <<spark-sql-streaming-StreamingDeduplicationStrategy.adoc#, StreamingDeduplicationStrategy>>
* <<spark-sql-streaming-StreamingGlobalLimitStrategy.adoc#, StreamingGlobalLimitStrategy>>

[[executedPlan]]
[NOTE]
====
`planner` is used to plan (aka _convert_) an optimized logical plan into a physical plan (that is later available as `sparkPlan`).

`sparkPlan` physical plan is then prepared for execution using <<preparations, preparations>> physical optimization rules. The result is later available as `executedPlan` physical plan.
====

| [[state]] `state`
a| State preparation rule (`Rule[SparkPlan]`) that transforms a streaming physical plan (i.e. `SparkPlan` with link:spark-sql-streaming-StateStoreSaveExec.adoc[StateStoreSaveExec], link:spark-sql-streaming-StreamingDeduplicateExec.adoc[StreamingDeduplicateExec] and link:spark-sql-streaming-FlatMapGroupsWithStateExec.adoc[FlatMapGroupsWithStateExec] physical operators) and fills missing properties that are batch-specific, e.g.

* `StateStoreSaveExec` and `StateStoreRestoreExec` operator pair, `state` assigns:

** `StateStoreSaveExec` operator gets <<nextStatefulOperationStateInfo, nextStatefulOperationStateInfo>>, <<outputMode, OutputMode>> and <<offsetSeqMetadata, batchWatermarkMs>>

** `StateStoreRestoreExec` operator gets <<nextStatefulOperationStateInfo, nextStatefulOperationStateInfo>> that was used for `StateStoreSaveExec` operator

* `StreamingDeduplicateExec` operator gets <<nextStatefulOperationStateInfo, nextStatefulOperationStateInfo>> and <<offsetSeqMetadata, batchWatermarkMs>>

* `FlatMapGroupsWithStateExec` gets <<nextStatefulOperationStateInfo, nextStatefulOperationStateInfo>>, <<offsetSeqMetadata, batchWatermarkMs>> and <<offsetSeqMetadata, batchWatermarkMs>>

Used when `IncrementalExecution` <<preparations, prepares a physical plan>> (i.e. `SparkPlan`) for execution (which is when `StreamExecution` link:spark-sql-streaming-MicroBatchExecution.adoc#runBatch-queryPlanning[runs a streaming batch and plans a streaming query]).

| [[statefulOperatorId]] `statefulOperatorId`
a| Java's `AtomicInteger`

* `0` when `IncrementalExecution` is <<creating-instance, created>>

* Incremented...FIXME
|===

=== [[nextStatefulOperationStateInfo]] `nextStatefulOperationStateInfo` Internal Method

[source, scala]
----
nextStatefulOperationStateInfo(): StatefulOperatorStateInfo
----

`nextStatefulOperationStateInfo` creates a new <<spark-sql-streaming-StatefulOperatorStateInfo.adoc#, StatefulOperatorStateInfo>> with <<checkpointLocation, checkpointLocation>>, <<runId, runId>>, the next <<statefulOperatorId, statefulOperatorId>> and <<currentBatchId, currentBatchId>>.

NOTE: All the properties of `StatefulOperatorStateInfo` are specified when `IncrementalExecution` is <<creating-instance, created>>.

NOTE: `nextStatefulOperationStateInfo` is used exclusively when `IncrementalExecution` is requested to transform a streaming physical plan using <<state, state>> preparation rule.

=== [[creating-instance]] Creating IncrementalExecution Instance

`IncrementalExecution` takes the following when created:

* [[sparkSession]] `SparkSession`
* [[logicalPlan]] Logical query plan (i.e. `LogicalPlan` link:spark-sql-streaming-MicroBatchExecution.adoc#runBatch-queryPlanning[with the logical plans of the data sources that have new data and new column attributes])
* [[outputMode]] link:spark-sql-streaming-OutputMode.adoc[OutputMode] (as specified using link:spark-sql-streaming-DataStreamWriter.adoc#outputMode[outputMode] method of `DataStreamWriter`)
* [[checkpointLocation]] `state` checkpoint directory
* [[runId]] Run id
* [[currentBatchId]] Current batch id
* [[offsetSeqMetadata]] <<spark-sql-streaming-OffsetSeqMetadata.adoc#, OffsetSeqMetadata>>

`IncrementalExecution` initializes the <<internal-registries, internal registries and counters>>.

=== [[shouldRunAnotherBatch]] `shouldRunAnotherBatch` Method

[source, scala]
----
shouldRunAnotherBatch(newMetadata: OffsetSeqMetadata): Boolean
----

`shouldRunAnotherBatch`...FIXME

NOTE: `shouldRunAnotherBatch` is used exclusively when `MicroBatchExecution` is requested to <<spark-sql-streaming-MicroBatchExecution.adoc#constructNextBatch, construct the next streaming micro-batch>>.

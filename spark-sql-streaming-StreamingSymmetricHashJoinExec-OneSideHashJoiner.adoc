== [[OneSideHashJoiner]] OneSideHashJoiner

`OneSideHashJoiner` is <<creating-instance, created>> exclusively when `StreamingSymmetricHashJoinExec` physical operator is requested to <<spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#doExecute, execute and generate a recipe for a distributed computation (as an RDD[InternalRow])>> (through <<spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#processPartitions, process partitions>> for the left and right sides of the stream-stream join).

NOTE: `OneSideHashJoiner` is a Scala private internal class of <<spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#, StreamingSymmetricHashJoinExec>>.

=== [[storeAndJoinWithOtherSide]] `storeAndJoinWithOtherSide` Method

[source, scala]
----
storeAndJoinWithOtherSide(
  otherSideJoiner: OneSideHashJoiner)(
  generateJoinedRow: (InternalRow, InternalRow) => JoinedRow): Iterator[InternalRow]
----

`storeAndJoinWithOtherSide`...FIXME

NOTE: `storeAndJoinWithOtherSide` is used when `StreamingSymmetricHashJoinExec` physical operator is requested to <<spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#processPartitions, process partitions>> (when requested to <<spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#doExecute, execute and generate a recipe for a distributed computation (as an RDD[InternalRow])>>).

=== [[creating-instance]] Creating OneSideHashJoiner Instance

`OneSideHashJoiner` takes the following to be created:

* [[joinSide]] <<spark-sql-streaming-SymmetricHashJoinStateManager.adoc#joinSide-internals, JoinSide>>
* [[inputAttributes]] Input attributes (`Seq[Attribute]`)
* [[joinKeys]] Join keys (`Seq[Expression]`)
* [[inputIter]] Input internal rows (`Iterator[InternalRow]`)
* [[preJoinFilterExpr]] Optional pre-join filter Catalyst expression
* [[postJoinFilter]] Post-join filter (`(InternalRow) => Boolean`)
* [[stateWatermarkPredicate]] Optional `JoinStateWatermarkPredicate`

`OneSideHashJoiner` initializes the <<internal-registries, internal registries and counters>>.

=== [[removeOldState]] `removeOldState` Method

[source, scala]
----
removeOldState(): Iterator[UnsafeRowPair]
----

`removeOldState` branches off per the <<stateWatermarkPredicate, JoinStateWatermarkPredicate>>:

* For `JoinStateKeyWatermarkPredicate`, `removeOldState` requests the <<joinStateManager, SymmetricHashJoinStateManager>> to <<spark-sql-streaming-SymmetricHashJoinStateManager.adoc#removeByKeyCondition, removeByKeyCondition>> (with the <<stateKeyWatermarkPredicateFunc, stateKeyWatermarkPredicateFunc>>)

* For `JoinStateValueWatermarkPredicate`, `removeOldState` requests the <<joinStateManager, SymmetricHashJoinStateManager>> to <<spark-sql-streaming-SymmetricHashJoinStateManager.adoc#removeByValueCondition, removeByValueCondition>> (with the <<stateValueWatermarkPredicateFunc, stateValueWatermarkPredicateFunc>>)

* For any other predicates, `removeOldState` returns an empty iterator (no rows to process)

NOTE: `removeOldState` is used exclusively when `StreamingSymmetricHashJoinExec` physical operator is requested to <<spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#processPartitions, process partitions (of the left and right side of a stream-stream join)>>.

=== [[get]] Getting Values For Key -- `get` Method

[source, scala]
----
get(key: UnsafeRow): Iterator[UnsafeRow]
----

`get`...FIXME

NOTE: `get` is used when...FIXME

=== [[commitStateAndGetMetrics]] `commitStateAndGetMetrics` Method

[source, scala]
----
commitStateAndGetMetrics(): StateStoreMetrics
----

`commitStateAndGetMetrics` simply requests the <<joinStateManager, SymmetricHashJoinStateManager>> to <<spark-sql-streaming-SymmetricHashJoinStateManager.adoc#commit, commit>> and then for the <<spark-sql-streaming-SymmetricHashJoinStateManager.adoc#metrics, metrics>>.

NOTE: `commitStateAndGetMetrics` is used exclusively when `StreamingSymmetricHashJoinExec` physical operator is requested to <<spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#processPartitions, process partitions>> (when requested to <<spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#doExecute, execute and generate a recipe for a distributed computation (as an RDD[InternalRow])>>).

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| joinStateManager
| [[joinStateManager]] <<spark-sql-streaming-SymmetricHashJoinStateManager.adoc#, SymmetricHashJoinStateManager>>

Used when...FIXME

| keyGenerator
| [[keyGenerator]] `UnsafeProjection` to generate join keys

Used when...FIXME

| preJoinFilter
| [[preJoinFilter]] `InternalRow => Boolean`

Used when...FIXME

| stateKeyWatermarkPredicateFunc
| [[stateKeyWatermarkPredicateFunc]] `InternalRow => Boolean`

Used when...FIXME

| stateValueWatermarkPredicateFunc
| [[stateValueWatermarkPredicateFunc]] `InternalRow => Boolean`

Used when...FIXME

| updatedStateRowsCount
a| [[updatedStateRowsCount]][[numUpdatedStateRows]] Counter

Used exclusively when requested to <<storeAndJoinWithOtherSide, storeAndJoinWithOtherSide>>
|===

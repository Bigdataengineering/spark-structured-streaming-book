== [[StreamExecution]] StreamExecution -- Base of Streaming Query Execution Engines

`StreamExecution` is the <<contract, base>> of <<extensions, streaming query execution engines>> that can <<runActivatedStream, run>> a <<logicalPlan, structured query>> at regular intervals on a <<queryExecutionThread, stream execution thread>>.

NOTE: *Continuous query*, *streaming query*, *continuous Dataset*, *streaming Dataset* can all be considered synonyms, and `StreamExecution` uses <<logicalPlan, analyzed logical plan>> internally to refer to it.

[[contract]]
.StreamExecution Contract
[cols="1m,3",options="header",width="100%"]
|===
| Property
| Description

| logicalPlan
a| [[logicalPlan]]

[source, scala]
----
logicalPlan: LogicalPlan
----

Analyzed logical plan of the streaming query to execute

Used when `StreamExecution` is requested to <<runStream, run stream processing>>

NOTE: `logicalPlan` is part of <<spark-sql-streaming-ProgressReporter.adoc#logicalPlan, ProgressReporter Contract>> and the only purpose of the `logicalPlan` property is to change the access level from `protected` to `public`.

| runActivatedStream
a| [[runActivatedStream]]

[source, scala]
----
runActivatedStream(sparkSessionForStream: SparkSession): Unit
----

Executes the activated streaming query

Used exclusively when `StreamExecution` is requested to <<runStream, run the streaming query>> (when transitioning from `INITIALIZING` to `ACTIVE` state)

|===

[[minLogEntriesToMaintain]][[spark.sql.streaming.minBatchesToRetain]]
`StreamExecution` uses the <<spark-sql-streaming-properties.adoc#spark.sql.streaming.minBatchesToRetain, spark.sql.streaming.minBatchesToRetain>> configuration property to allow the <<extensions, StreamExecutions>> to discard old log entries (from the <<offsetLog, offset>> and <<commitLog, commit>> logs).

[[extensions]]
.StreamExecutions
[cols="30,70",options="header",width="100%"]
|===
| StreamExecution
| Description

| <<spark-sql-streaming-ContinuousExecution.adoc#, ContinuousExecution>>
| [[ContinuousExecution]] Used in <<spark-sql-streaming-continuous-stream-processing.adoc#, Continuous Stream Processing>>

| <<spark-sql-streaming-MicroBatchExecution.adoc#, MicroBatchExecution>>
| [[MicroBatchExecution]] Used in <<spark-sql-streaming-micro-batch-processing.adoc#, Micro-Batch Stream Processing>>
|===

NOTE: `StreamExecution` does not support adaptive query execution and cost-based optimizer (and turns them off when requested to <<runStream, run stream processing>>).

`StreamExecution` is the *execution environment* of a link:spark-sql-streaming-StreamingQuery.adoc[single streaming query] (aka _streaming Dataset_) that is executed every <<trigger, trigger>> and in the end <<runBatch-addBatch, adds the results to a sink>>.

NOTE: `StreamExecution` corresponds to a link:spark-sql-streaming-StreamingQuery.adoc[single streaming query] with one or more link:spark-sql-streaming-Source.adoc[streaming sources] and exactly one link:spark-sql-streaming-Sink.adoc[streaming sink].

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val q = spark.
  readStream.
  format("rate").
  load.
  writeStream.
  format("console").
  trigger(Trigger.ProcessingTime(10.minutes)).
  start
scala> :type q
org.apache.spark.sql.streaming.StreamingQuery

// Pull out StreamExecution off StreamingQueryWrapper
import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper}
val se = q.asInstanceOf[StreamingQueryWrapper].streamingQuery
scala> :type se
org.apache.spark.sql.execution.streaming.StreamExecution
----

.Creating Instance of StreamExecution
image::images/StreamExecution-creating-instance.png[align="center"]

NOTE: link:spark-sql-streaming-DataStreamWriter.adoc[DataStreamWriter] describes how the results of executing batches of a streaming query are written to a streaming sink.

When <<start, started>>, `StreamExecution` starts a <<queryExecutionThread, stream execution thread>> that simply <<runStream, runs stream processing>> (and hence the streaming query).

.StreamExecution's Starting Streaming Query (on Execution Thread)
image::images/StreamExecution-start.png[align="center"]

`StreamExecution` can be in three <<state, states>>:

* `INITIALIZED` when the instance was created.
* `ACTIVE` when batches are pulled from the sources.
* `TERMINATED` when executing streaming batches has been terminated due to an error, all batches were successfully processed or `StreamExecution` <<stop, has been stopped>>.

`StreamExecution` is a link:spark-sql-streaming-ProgressReporter.adoc[ProgressReporter] and <<postEvent, reports status of the streaming query>> (i.e. when it starts, progresses and terminates) by posting `StreamingQueryListener` events.

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val sq = spark
  .readStream
  .text("server-logs")
  .writeStream
  .format("console")
  .queryName("debug")
  .trigger(Trigger.ProcessingTime(20.seconds))
  .start

// Enable the log level to see the INFO and DEBUG messages
// log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG

17/06/18 21:21:07 INFO StreamExecution: Starting new streaming query.
17/06/18 21:21:07 DEBUG StreamExecution: getOffset took 5 ms
17/06/18 21:21:07 DEBUG StreamExecution: Stream running from {} to {}
17/06/18 21:21:07 DEBUG StreamExecution: triggerExecution took 9 ms
17/06/18 21:21:07 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())
17/06/18 21:21:07 INFO StreamExecution: Streaming query made progress: {
  "id" : "8b57b0bd-fc4a-42eb-81a3-777d7ba5e370",
  "runId" : "920b227e-6d02-4a03-a271-c62120258cea",
  "name" : "debug",
  "timestamp" : "2017-06-18T19:21:07.693Z",
  "numInputRows" : 0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 9
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Users/jacek/dev/oss/spark/server-logs]",
    "startOffset" : null,
    "endOffset" : null,
    "numInputRows" : 0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2460208a"
  }
}
17/06/18 21:21:10 DEBUG StreamExecution: Starting Trigger Calculation
17/06/18 21:21:10 DEBUG StreamExecution: getOffset took 3 ms
17/06/18 21:21:10 DEBUG StreamExecution: triggerExecution took 3 ms
17/06/18 21:21:10 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())
----

`StreamExecution` tracks streaming data sources in <<uniqueSources, uniqueSources>> internal registry.

.StreamExecution's uniqueSources Registry of Streaming Data Sources
image::images/StreamExecution-uniqueSources.png[align="center"]

`StreamExecution` collects `durationMs` for the execution units of streaming batches.

.StreamExecution's durationMs
image::images/StreamExecution-durationMs.png[align="center"]

[source, scala]
----
scala> :type q
org.apache.spark.sql.streaming.StreamingQuery

scala> println(q.lastProgress)
{
  "id" : "03fc78fc-fe19-408c-a1ae-812d0e28fcee",
  "runId" : "8c247071-afba-40e5-aad2-0e6f45f22488",
  "name" : null,
  "timestamp" : "2017-08-14T20:30:00.004Z",
  "batchId" : 1,
  "numInputRows" : 432,
  "inputRowsPerSecond" : 0.9993568953312452,
  "processedRowsPerSecond" : 1380.1916932907347,
  "durationMs" : {
    "addBatch" : 237,
    "getBatch" : 26,
    "getOffset" : 0,
    "queryPlanning" : 1,
    "triggerExecution" : 313,
    "walCommit" : 45
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]",
    "startOffset" : 0,
    "endOffset" : 432,
    "numInputRows" : 432,
    "inputRowsPerSecond" : 0.9993568953312452,
    "processedRowsPerSecond" : 1380.1916932907347
  } ],
  "sink" : {
    "description" : "ConsoleSink[numRows=20, truncate=true]"
  }
}
----

`StreamExecution` uses <<offsetLog, OffsetSeqLog>> and <<batchCommitLog, BatchCommitLog>> metadata logs for *write-ahead log* (to record offsets to be processed) and that have already been processed and committed to a streaming sink, respectively.

TIP: Monitor `offsets` and `commits` metadata logs to know the progress of a streaming query.

`StreamExecution` <<runBatches-batchRunner-no-data, delays polling for new data>> for 10 milliseconds (when no data was available to process in a batch). Use link:spark-sql-streaming-properties.adoc#spark.sql.streaming.pollingDelay[spark.sql.streaming.pollingDelay] Spark property to control the delay.

[[id]]
Every `StreamExecution` is uniquely identified by an *ID of the streaming query* (which is the `id` of the <<streamMetadata, StreamMetadata>>).

NOTE: Since the <<streamMetadata, StreamMetadata>> is persisted (to the `metadata` file in the <<checkpointFile, checkpoint directory>>), the streaming query ID "survives" query restarts as long as the checkpoint directory is preserved.

[[runId]]
`StreamExecution` is also uniquely identified by a *run ID of the streaming query*. A run ID is a randomly-generated 128-bit universally unique identifier (UUID) that is assigned at the time `StreamExecution` is created.

NOTE: `runId` does not "survive" query restarts and will always be different yet unique (across all active queries).

[NOTE]
====
The <<name, name>>, <<id, id>> and <<runId, runId>> are all unique across all active queries (in a <<spark-sql-streaming-StreamingQueryManager.adoc#, StreamingQueryManager>>). The difference is that:

* <<name, name>> is optional and user-defined

* <<id, id>> is a UUID that is auto-generated at the time `StreamExecution` is created and persisted to `metadata` checkpoint file

* <<runId, runId>> is a UUID that is auto-generated every time `StreamExecution` is created
====

[[streamMetadata]]
`StreamExecution` uses a <<spark-sql-streaming-StreamMetadata.adoc#, StreamMetadata>> that is <<spark-sql-streaming-StreamMetadata.adoc#write, persisted>> in the `metadata` file in the <<checkpointFile, checkpoint directory>>. If the `metadata` file is available it is <<spark-sql-streaming-StreamMetadata.adoc#read, read>> and is the way to recover the <<id, ID>> of a streaming query when resumed (i.e. restarted after a failure or a planned stop).

[[IS_CONTINUOUS_PROCESSING]]
`StreamExecution` uses *__is_continuous_processing* local property (default: `false`) to differentiate between <<spark-sql-streaming-ContinuousExecution.adoc#, ContinuousExecution>> (`true`) and <<spark-sql-streaming-MicroBatchExecution.adoc#, MicroBatchExecution>> (`false`) which is used when `StateStoreRDD` is requested to <<spark-sql-streaming-StateStoreRDD.adoc#compute, compute a partition>> (and <<spark-sql-streaming-StateStore.adoc#get, finds a StateStore>> for a given version).

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.spark.sql.execution.streaming.StreamExecution` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=ALL
```

Refer to <<spark-sql-streaming-logging.adoc#, Logging>>.
====

=== [[resolvedCheckpointRoot]] Fully-Qualified (Resolved) Path to Checkpoint Root Directory -- `resolvedCheckpointRoot` Property

[source, scala]
----
resolvedCheckpointRoot: String
----

`resolvedCheckpointRoot` is a fully-qualified path of the given <<checkpointRoot, checkpoint root directory>>.

The given <<checkpointRoot, checkpoint root directory>> is defined using *checkpointLocation* option or the <<spark-sql-streaming-properties.adoc#spark.sql.streaming.checkpointLocation, spark.sql.streaming.checkpointLocation>> configuration property with `queryName` option.

`checkpointLocation` and `queryName` options are defined when `StreamingQueryManager` is requested to <<spark-sql-streaming-StreamingQueryManager.adoc#createQuery, create a streaming query>>.

`resolvedCheckpointRoot` is used when <<checkpointFile, creating the path to the checkpoint directory>> and when `StreamExecution` finishes <<runBatches, running streaming batches>>.

`resolvedCheckpointRoot` is used for the <<logicalPlan, logicalPlan>> (while transforming <<analyzedPlan, analyzedPlan>> and planning `StreamingRelation` logical operators to corresponding `StreamingExecutionRelation` physical operators with the streaming data sources created passing in the path to `sources` directory to store checkpointing metadata).

[TIP]
====
You can see `resolvedCheckpointRoot` in the INFO message when `StreamExecution` is <<start, started>>.

```
Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint.
```
====

Internally, `resolvedCheckpointRoot` creates a Hadoop `org.apache.hadoop.fs.Path` for <<checkpointRoot, checkpointRoot>> and makes it qualified.

NOTE: `resolvedCheckpointRoot` uses `SparkSession` to access `SessionState` for a Hadoop configuration.

=== [[commitLog]] Offset Commit Log -- `commits` Metadata Checkpoint Directory

`StreamExecution` uses *offset commit log* (<<spark-sql-streaming-CommitLog.adoc#, CommitLog>> with `commits` <<checkpointFile, metadata checkpoint directory>>) for completed streaming batches (with a single file per batch with a file name being the batch id) or committed epochs.

NOTE: *Metadata log* or *metadata checkpoint* are synonyms and are often used interchangeably.

`commitLog` is used by the <<extensions, extensions>> for the following:

* `MicroBatchExecution` is requested to <<spark-sql-streaming-MicroBatchExecution.adoc#runActivatedStream, run an activated streaming query>> (that in turn requests to <<spark-sql-streaming-MicroBatchExecution.adoc#populateStartOffsets, populate the start offsets>> at the very beginning of the query execution and later regularly every <<spark-sql-streaming-MicroBatchExecution.adoc#runBatch, single batch>>)

* `ContinuousExecution` is requested to <<spark-sql-streaming-ContinuousExecution.adoc#runActivatedStream, run an activated streaming query in continuous mode>> (that in turn requests to <<spark-sql-streaming-ContinuousExecution.adoc#getStartOffsets, retrieve the start offsets>> at the very beginning of the query execution and later regularly every <<spark-sql-streaming-ContinuousExecution.adoc#commit, commit>>)

=== [[stopSources]] `stopSources` Method

[source, scala]
----
stopSources(): Unit
----

`stopSources`...FIXME

[NOTE]
====
`stopSources` is used when:

* `StreamExecution` is requested to <<runStream, run stream processing>> (and terminates)

* `ContinuousExecution` is requested to <<spark-sql-streaming-ContinuousExecution.adoc#runContinuous, run the streaming query in continuous mode>> (and terminates)
====

=== [[runStream]] Running Stream Processing -- `runStream` Internal Method

[source, scala]
----
runStream(): Unit
----

`runStream` simply prepares the environment to <<runActivatedStream, execute the activated streaming query>>.

NOTE: `runStream` is used exclusively when the <<queryExecutionThread, stream execution thread>> is requested to start (when `DataStreamWriter` is requested to <<spark-sql-streaming-DataStreamWriter.adoc#start, start an execution of the streaming query>>).

Internally, `runStream` sets the job group (to all the Spark jobs started by this thread) as follows:

* <<runId, runId>> for the job group ID

* <<getBatchDescriptionString, getBatchDescriptionString>> for the job group description (to display in web UI)

* `interruptOnCancel` flag on

[NOTE]
====
`runStream` uses the <<sparkSession, SparkSession>> to access `SparkContext` and assign the job group id.

Read up on https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-SparkContext.html#setJobGroup[SparkContext.setJobGroup] method in https://bit.ly/apache-spark-internals[The Internals of Apache Spark] book.
====

`runStream` sets `sql.streaming.queryId` local property to <<id, id>>.

`runStream` requests the `MetricsSystem` to register the <<streamMetrics, MetricsReporter>> when <<spark-sql-streaming-properties.adoc#spark.sql.streaming.metricsEnabled, spark.sql.streaming.metricsEnabled>> configuration property is on (default: off / `false`).

`runStream` notifies <<spark-sql-streaming-StreamingQueryListener.adoc#, StreamingQueryListeners>> that the streaming query has been started (by <<postEvent, posting>> a new <<spark-sql-streaming-StreamingQueryListener.adoc#QueryStartedEvent, QueryStartedEvent>> event with <<id, id>>, <<runId, runId>>, and <<name, name>>).

.StreamingQueryListener Notified about Query's Start (onQueryStarted)
image::images/StreamingQueryListener-onQueryStarted.png[align="center"]

`runStream` unblocks the <<start, main starting thread>> (by decrementing the count of the <<startLatch, startLatch>> that when `0` lets the starting thread continue).

CAUTION: FIXME A picture with two parallel lanes for the starting thread and daemon one for the query.

`runStream` <<spark-sql-streaming-ProgressReporter.adoc#updateStatusMessage, updates the status message>> to be *Initializing sources*.

[[runStream-initializing-sources]]
`runStream` initializes the <<logicalPlan, analyzed logical plan>>.

NOTE: The <<logicalPlan, analyzed logical plan>> is a lazy value in Scala and is initialized when requested the very first time.

`runStream` disables *adaptive query execution* and *cost-based join optimization* (by turning `spark.sql.adaptive.enabled` and `spark.sql.cbo.enabled` configuration properties off, respectively).

`runStream` creates a new "zero" <<offsetSeqMetadata, OffsetSeqMetadata>>.

(Only when in <<state, INITIALIZING>> state) `runStream` enters <<state, ACTIVE>> state:

* Decrements the count of <<initializationLatch, initializationLatch>>

* [[runStream-runActivatedStream]] <<runActivatedStream, Executes the activated streaming query>> (which is different per <<extensions, StreamExecution>>, i.e. <<spark-sql-streaming-ContinuousExecution.adoc#, ContinuousExecution>> or <<spark-sql-streaming-MicroBatchExecution.adoc#, MicroBatchExecution>>).

NOTE: `runBatches` does the main work only when first started (i.e. when <<state, state>> is `INITIALIZING`).

[[runStream-stopped]]
`runStream`...FIXME (describe the failed and stop states)

Once <<triggerExecutor, TriggerExecutor>> has finished executing batches, `runBatches` link:spark-sql-streaming-ProgressReporter.adoc#updateStatusMessage[updates the status message] to *Stopped*.

NOTE: <<triggerExecutor, TriggerExecutor>> finishes executing batches when <<runBatches-batch-runner, batch runner>> returns whether the streaming query is stopped or not (which is when the internal <<state, state>> is not `TERMINATED`).

[[runBatches-catch-isInterruptedByStop]]
[[runBatches-catch-IOException]]
[[runBatches-catch-Throwable]]
CAUTION: FIXME Describe `catch` block for exception handling

[[runStream-finally]]
CAUTION: FIXME Describe `finally` block for query termination

==== [[runBatches-batch-runner]] TriggerExecutor's Batch Runner

*Batch Runner* (aka `batchRunner`) is an executable block executed by <<triggerExecutor, TriggerExecutor>> in <<runBatches, runBatches>>.

`batchRunner` <<startTrigger, starts trigger calculation>>.

As long as the query is not stopped (i.e. <<state, state>> is not `TERMINATED`), `batchRunner` executes the streaming batch for the trigger.

In *triggerExecution* link:spark-sql-streaming-ProgressReporter.adoc#reportTimeTaken[time-tracking section], `runBatches` branches off per <<currentBatchId, currentBatchId>>.

.Current Batch Execution per currentBatchId
[cols="1,1",options="header",width="100%"]
|===
| currentBatchId < 0
| currentBatchId >= 0

a|

1. <<populateStartOffsets, populateStartOffsets>>
1. Setting Job Description as <<getBatchDescriptionString, getBatchDescriptionString>>

```
DEBUG Stream running from [committedOffsets] to [availableOffsets]
```

| 1. <<constructNextBatch, Constructing the next streaming micro-batch>>
|===

If there is <<dataAvailable, data available>> in the sources, `batchRunner` marks <<currentStatus, currentStatus>> with `isDataAvailable` enabled.

[NOTE]
====
You can check out the status of a link:spark-sql-streaming-StreamingQuery.adoc[streaming query] using link:spark-sql-streaming-StreamingQuery.adoc#status[status] method.

[source, scala]
----
scala> spark.streams.active(0).status
res1: org.apache.spark.sql.streaming.StreamingQueryStatus =
{
  "message" : "Waiting for next trigger",
  "isDataAvailable" : false,
  "isTriggerActive" : false
}
----
====

`batchRunner` then link:spark-sql-streaming-ProgressReporter.adoc#updateStatusMessage[updates the status message] to *Processing new data* and <<runBatch, runs the current streaming batch>>.

.StreamExecution's Running Batches (on Execution Thread)
image::images/StreamExecution-runBatches.png[align="center"]

[[runBatches-batch-runner-finishTrigger]]
After *triggerExecution* section has finished, `batchRunner` link:spark-sql-streaming-ProgressReporter.adoc#finishTrigger[finishes the streaming batch for the trigger] (and collects query execution statistics).

When there was <<dataAvailable, data available>> in the sources, `batchRunner` updates committed offsets (by link:spark-sql-streaming-CommitLog.adoc#add[adding] the <<currentBatchId, current batch id>> to <<batchCommitLog, BatchCommitLog>> and adding <<availableOffsets, availableOffsets>> to <<committedOffsets, committedOffsets>>).

You should see the following DEBUG message in the logs:

```
DEBUG batch $currentBatchId committed
```

`batchRunner` increments the <<currentBatchId, current batch id>> and sets the job description for all the following Spark jobs to <<getBatchDescriptionString, include the new batch id>>.

[[runBatches-batchRunner-no-data]]
When no <<dataAvailable, data was available>> in the sources to process, `batchRunner` does the following:

1. Marks <<currentStatus, currentStatus>> with `isDataAvailable` disabled

1. link:spark-sql-streaming-ProgressReporter.adoc#updateStatusMessage[Updates the status message] to *Waiting for data to arrive*

1. Sleeps the current thread for <<pollingDelayMs, pollingDelayMs>> milliseconds.

`batchRunner` link:spark-sql-streaming-ProgressReporter.adoc#updateStatusMessage[updates the status message] to *Waiting for next trigger* and returns whether the query is currently active or not (so <<triggerExecutor, TriggerExecutor>> can decide whether to finish executing the batches or not)

=== [[getBatchDescriptionString]] `getBatchDescriptionString` Method

[source, scala]
----
getBatchDescriptionString: String
----

`getBatchDescriptionString`...FIXME

[NOTE]
====
`getBatchDescriptionString` is used when:

* `MicroBatchExecution` is requested to <<spark-sql-streaming-MicroBatchExecution.adoc#runActivatedStream, runActivatedStream>> (and sets the job description)

* `StreamExecution` is requested to <<runStream, runStream>> (and sets job group)
====

=== [[start]] Starting Streaming Query (on Stream Execution Thread) -- `start` Method

[source, scala]
----
start(): Unit
----

When called, `start` prints out the following INFO message to the logs:

```
Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint.
```

`start` then starts the <<queryExecutionThread, stream execution thread>> (as a daemon thread).

NOTE: `start` uses Java's link:++https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#start--++[java.lang.Thread.start] to run the streaming query on a separate execution thread.

NOTE: When started, a streaming query runs in its own execution thread on JVM.

In the end, `start` pauses the main thread (using the <<startLatch, startLatch>> until `StreamExecution` is requested to <<runStream, run the streaming query>> that in turn sends a <<spark-sql-streaming-StreamingQueryListener.adoc#QueryStartedEvent, QueryStartedEvent>> to all streaming listeners followed by decrementing the count of the <<startLatch, startLatch>>).

NOTE: `start` is used exclusively when `StreamingQueryManager` is requested to <<spark-sql-streaming-StreamingQueryManager.adoc#startQuery, start a streaming query>> (when `DataStreamWriter` is requested to <<spark-sql-streaming-DataStreamWriter.adoc#start, start an execution of the streaming query>>).

=== [[creating-instance]] Creating StreamExecution Instance

`StreamExecution` takes the following when created:

* [[sparkSession]] `SparkSession`
* [[name]] Name of the streaming query (can also be `null`)
* [[checkpointRoot]] Path of the checkpoint directory (aka _metadata directory_)
* [[analyzedPlan]] Analyzed logical query plan (i.e. `LogicalPlan`)
* [[sink]] <<spark-sql-streaming-Sink.adoc#, Streaming sink>>
* [[trigger]] <<spark-sql-streaming-Trigger.adoc#, Trigger>>
* [[triggerClock]] `Clock`
* [[outputMode]] <<spark-sql-streaming-OutputMode.adoc#, Output mode>> (that is only used when creating `IncrementalExecution` for a streaming batch in <<runBatch-queryPlanning, query planning>>)
* [[deleteCheckpointOnStop]] `deleteCheckpointOnStop` flag to control whether to delete the checkpoint directory on stop

`StreamExecution` initializes the <<internal-properties, internal properties>>.

NOTE: `StreamExecution` is a Scala abstract class and cannot be <<creating-instance, created>> directly. It is created indirectly when the <<extensions, concrete StreamExecutions>> are.

=== [[checkpointFile]] Path to Checkpoint Directory -- `checkpointFile` Internal Method

[source, scala]
----
checkpointFile(name: String): String
----

`checkpointFile` gives the path of a directory with `name` in <<resolvedCheckpointRoot, checkpoint directory>>.

NOTE: `checkpointFile` uses Hadoop's `org.apache.hadoop.fs.Path`.

NOTE: `checkpointFile` is used for <<streamMetadata, streamMetadata>>, <<offsetLog, OffsetSeqLog>>, <<batchCommitLog, BatchCommitLog>>, and <<lastExecution, lastExecution>> (for <<runBatch, runBatch>>).

=== [[postEvent]] Posting StreamingQueryListener Event -- `postEvent` Method

[source, scala]
----
postEvent(event: StreamingQueryListener.Event): Unit
----

NOTE: `postEvent` is a part of link:spark-sql-streaming-ProgressReporter.adoc#postEvent[ProgressReporter Contract].

`postEvent` simply requests the `StreamingQueryManager` to link:spark-sql-streaming-StreamingQueryManager.adoc#postListenerEvent[post] the input event (to the link:spark-sql-streaming-StreamingQueryListenerBus.adoc[StreamingQueryListenerBus] in the current `SparkSession`).

NOTE: `postEvent` uses `SparkSession` to access the current `StreamingQueryManager`.

[NOTE]
====
`postEvent` is used when:

* `ProgressReporter` link:spark-sql-streaming-ProgressReporter.adoc#updateProgress[reports update progress] (while link:spark-sql-streaming-ProgressReporter.adoc#finishTrigger[finishing a trigger])

* `StreamExecution` <<runBatches, runs streaming batches>> (and announces starting a streaming query by posting a link:spark-sql-streaming-StreamingQueryListener.adoc#QueryStartedEvent[QueryStartedEvent] and query termination by posting a link:spark-sql-streaming-StreamingQueryListener.adoc#QueryTerminatedEvent[QueryTerminatedEvent])
====

=== [[processAllAvailable]] Waiting Until No Data Available in Sources or Query Has Been Terminated -- `processAllAvailable` Method

[source, scala]
----
processAllAvailable(): Unit
----

NOTE: `processAllAvailable` is a part of link:spark-sql-streaming-StreamingQuery.adoc#processAllAvailable[StreamingQuery Contract].

`processAllAvailable` reports <<streamDeathCause, streamDeathCause>> exception if defined (and returns).

NOTE: <<streamDeathCause, streamDeathCause>> is defined exclusively when `StreamExecution` <<runBatches, runs streaming batches>> (and terminated with an exception).

`processAllAvailable` returns when <<isActive, isActive>> flag is turned off (which is when `StreamExecution` is in `TERMINATED` state).

`processAllAvailable` acquires a lock on <<awaitProgressLock, awaitProgressLock>> and turns <<noNewData, noNewData>> flag off.

`processAllAvailable` keeps waiting 10 seconds for <<awaitProgressLockCondition, awaitProgressLockCondition>> until <<noNewData, noNewData>> flag is turned on or `StreamExecution` is no longer <<isActive, active>>.

NOTE: <<noNewData, noNewData>> flag is turned on exclusively when `StreamExecution` <<constructNextBatch, constructs the next streaming micro-batch>> (and finds that <<constructNextBatch-hasNewData-false, no data is available>>).

In the end, `processAllAvailable` releases <<awaitProgressLock, awaitProgressLock>> lock.

=== [[queryExecutionThread]] Stream Execution Thread -- `queryExecutionThread` Property

[source, scala]
----
queryExecutionThread: QueryExecutionThread
----

`queryExecutionThread` is a Java thread of execution (https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.util.Thread]) that <<runStream, runs the structured query>> when started.

`queryExecutionThread` uses the name *stream execution thread for [id]* (that uses <<prettyIdString, prettyIdString>> for the id, i.e. `queryName [id = [id], runId = [runId]]`).

`queryExecutionThread` is a `QueryExecutionThread` that is really a custom `UninterruptibleThread` from Apache Spark with `runUninterruptibly` method for running a block of code without being interrupted by `Thread.interrupt()`).

`queryExecutionThread` is started (as a daemon thread) when `StreamExecution` is requested to <<start, start>>.

When started, `queryExecutionThread` sets the thread-local properties as the <<callSite, call site>> and <<runBatches, runs the streaming query>>.

[TIP]
====
Use Java's http://docs.oracle.com/javase/8/docs/technotes/guides/management/jconsole.html[jconsole] or https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html[jstack] to monitor the streaming threads.

```
$ jstack <driver-pid> | grep -e "stream execution thread"
"stream execution thread for kafka-topic1 [id =...
```
====

=== [[toDebugString]] `toDebugString` Internal Method

[source, scala]
----
toDebugString(includeLogicalPlan: Boolean): String
----

`toDebugString`...FIXME

NOTE: `toDebugString` is used exclusively when `StreamExecution` is requested to <<runStream, run stream processing>> (and an exception is caught).

=== [[offsetSeqMetadata]] Current Batch Metadata (Event-Time Watermark and Timestamp) -- `offsetSeqMetadata` Internal Property

[source, scala]
----
offsetSeqMetadata: OffsetSeqMetadata
----

`offsetSeqMetadata` is a <<spark-sql-streaming-OffsetSeqMetadata.adoc#, OffsetSeqMetadata>>.

NOTE: `offsetSeqMetadata` is part of the <<spark-sql-streaming-ProgressReporter.adoc#offsetSeqMetadata, ProgressReporter Contract>> to hold the current event-time watermark and timestamp.

`offsetSeqMetadata` is used to create an <<spark-sql-streaming-IncrementalExecution.adoc#, IncrementalExecution>> in the *queryPlanning* phase of the <<spark-sql-streaming-MicroBatchExecution.adoc#runBatch-queryPlanning, MicroBatchExecution>> and <<spark-sql-streaming-ContinuousExecution.adoc#runContinuous-queryPlanning, ContinuousExecution>> execution engines.

`offsetSeqMetadata` is initialized (with `0` for `batchWatermarkMs` and `batchTimestampMs`) when `StreamExecution` is requested to <<runStream, run stream processing>>.

`offsetSeqMetadata` is then updated (with the current event-time watermark and timestamp) when `MicroBatchExecution` is requested to <<spark-sql-streaming-MicroBatchExecution.adoc#constructNextBatch, construct the next streaming micro-batch>>.

NOTE: `MicroBatchExecution` uses the <<spark-sql-streaming-MicroBatchExecution.adoc#watermarkTracker, WatermarkTracker>> for the current event-time watermark and the <<spark-sql-streaming-MicroBatchExecution.adoc#triggerClock, trigger clock>> for the current batch timestamp.

`offsetSeqMetadata` is stored (_checkpointed_) in the *walCommit* phase of <<spark-sql-streaming-MicroBatchExecution.adoc#walCommit, MicroBatchExecution>> (and printed out as INFO message to the logs).

```
FIXME INFO message
```

`offsetSeqMetadata` is restored (_re-created_) from a checkpointed state when `MicroBatchExecution` is requested to <<spark-sql-streaming-MicroBatchExecution.adoc#populateStartOffsets, populate start offsets>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| availableOffsets
a| [[availableOffsets]] <<spark-sql-streaming-StreamProgress.adoc#, StreamProgress>> that tracks the offsets that are available to be processed, but have not yet be committed to the sink.

NOTE: `availableOffsets` is part of the <<spark-sql-streaming-ProgressReporter.adoc#availableOffsets, ProgressReporter Contract>>.

NOTE: link:spark-sql-streaming-StreamProgress.adoc[StreamProgress] is an enhanced `immutable.Map` from Scala with streaming sources as keys and their link:spark-sql-streaming-Offset.adoc[Offsets] as values.

---

Set when (in order):

1. `StreamExecution` resumes and <<populateStartOffsets, populates the start offsets>> with the latest offsets from the <<offsetLog, offset log>> that may have already been processed (and committed to the <<batchCommitLog, batch commit log>> so they are used as the current <<committedOffsets, committed offsets>>)

1. `StreamExecution` <<constructNextBatch, constructs the next streaming micro-batch>> (and gets offsets from the sources)

[NOTE]
====
You can see <<availableOffsets, availableOffsets>> in DEBUG messages in the logs when `StreamExecution` resumes and <<populateStartOffsets, populates the start offsets>>.

[options="wrap"]
----
Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets]
----
====

Used when:

* `StreamExecution` starts <<runBatches, running streaming batches>> for the first time (i.e. <<currentBatchId, current batch id>> is `-1` which is right at the initialization time)

* `StreamExecution` <<dataAvailable, checks whether a new data is available in the sources>> (and is not recorded in <<committedOffsets, committed offsets>>)

* `StreamExecution` <<constructNextBatch, constructs the next streaming micro-batch>> (and records offsets in the <<offsetLog, write-ahead offset log>>)

* `StreamExecution` <<runBatch, runs a streaming batch>> (and fetches data from the sources that has not been processed yet, i.e. not in <<committedOffsets, committed offsets>> registry)

* `StreamExecution` finishes <<runBatches, running streaming batches>> when data was available in the sources and the offsets have just been committed to a sink (and being added to <<committedOffsets, committed offsets>> registry)

* `StreamExecution` <<toDebugString, prints out debug information>> when a streaming query has terminated due to an exception

NOTE: `availableOffsets` works in tandem with <<committedOffsets, committedOffsets>> registry.

| awaitProgressLock
| [[awaitProgressLock]] Java's fair reentrant mutual exclusion https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/ReentrantLock.html[java.util.concurrent.locks.ReentrantLock] (that favors granting access to the longest-waiting thread under contention)

| awaitProgressLockCondition
| [[awaitProgressLockCondition]]

| callSite
| [[callSite]]

| committedOffsets
a| [[committedOffsets]] *Committed offsets* (<<spark-sql-streaming-StreamProgress.adoc#, StreamProgress>> of pairs of the streaming sources and the offsets they already processed)

`committedOffsets` is a part of the link:spark-sql-streaming-ProgressReporter.adoc#committedOffsets[ProgressReporter Contract]

| currentBatchId
a| [[currentBatchId]] Current batch number

* `-1` when `StreamExecution` is <<creating-instance, created>>

* `0` when `StreamExecution` <<populateStartOffsets, populates start offsets>> (and <<offsetLog, OffsetSeqLog>> is empty, i.e. no offset files in `offsets` directory in checkpoint)

* Incremented when `StreamExecution` <<runBatches, runs streaming batches>> and finishes a trigger that had <<dataAvailable, data available from sources>> (right after <<batchCommitLog, committing the batch>>).

| initializationLatch
| [[initializationLatch]]

| lastExecution
| [[lastExecution]] Last link:spark-sql-streaming-IncrementalExecution.adoc[IncrementalExecution]

| newData
a| [[newData]]

[source, scala]
----
newData: Map[BaseStreamingSource, LogicalPlan]
----

Registry of the <<spark-sql-streaming-BaseStreamingSource.adoc#, streaming sources>> (in the <<logicalPlan, logical query plan>>) that have new data available in the current batch. The new data is a streaming `DataFrame`.

NOTE: `newData` is part of the <<spark-sql-streaming-ProgressReporter.adoc#newData, ProgressReporter Contract>>.

Set exclusively when `StreamExecution` is requested to <<runBatch-getBatch, requests unprocessed data from streaming sources>> (while <<runBatch, running a single streaming batch>>).

Used exclusively when `StreamExecution` <<runBatch-withNewSources, replaces StreamingExecutionRelations in a logical query plan with relations with new data>> (while <<runBatch, running a single streaming batch>>).

| noNewData
| [[noNewData]] Flag that indicates whether there are new offsets (_data_) available for processing or not

Turned on (i.e. enabled) when <<constructNextBatch, constructing the next streaming micro-batch>> when no new offsets are available.

| offsetLog
a| [[offsetLog]] *Offset write-ahead log* (<<spark-sql-streaming-OffsetSeqLog.adoc#, OffsetSeqLog>> with `offsets` <<checkpointFile, metadata checkpoint directory>>) to record offsets in when ready for processing.

NOTE: *Metadata log* or *metadata checkpoint* are synonyms and are often used interchangeably.

Used when `StreamExecution` <<populateStartOffsets, populates the start offsets>> and <<constructNextBatch, constructs the next streaming micro-batch>> (first to store the current batch's offsets in a write-ahead log and retrieve the previous batch's offsets right afterwards).

NOTE: `StreamExecution` <<constructNextBatch-purge, discards offsets from the offset metadata log>> when the <<currentBatchId, current batch id>> is above link:spark-sql-streaming-properties.adoc#spark.sql.streaming.minBatchesToRetain[spark.sql.streaming.minBatchesToRetain] Spark property (which defaults to `100`).

| pollingDelayMs
| [[pollingDelayMs]] Time delay before polling new data again when no data was available

Set to link:spark-sql-streaming-properties.adoc#spark.sql.streaming.pollingDelay[spark.sql.streaming.pollingDelay] Spark property.

Used when `StreamExecution` has started <<runBatches, running streaming batches>> (and <<runBatches-batchRunner-no-data, no data was available to process in a trigger>>).

| prettyIdString
a| [[prettyIdString]] Pretty-identified string for identification in logs (with <<name, name>> if defined).

```
// query name set
queryName [id = xyz, runId = abc]

// no query name
[id = xyz, runId = abc]
```

| sources
| [[sources]] All link:spark-sql-streaming-Source.adoc[streaming Sources] in <<logicalPlan, logical query plan>> (that are the link:spark-sql-streaming-StreamingExecutionRelation.adoc#source[sources] from `StreamingExecutionRelation`).

| startLatch
| [[startLatch]] Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountDownLatch.html[java.util.concurrent.CountDownLatch] with count `1`.

Used when `StreamExecution` is requested to <<start, start>> to pause the main thread until `StreamExecution` was requested to <<runStream, run the streaming query>>.

| state
a| [[state]] Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicReference.html[java.util.concurrent.atomic.AtomicReference] for the state of a streaming query execution:

* `INITIALIZING` (default)
* [[ACTIVE]] `ACTIVE` (after the first execution of <<runBatches, runBatches>>)
* `TERMINATED`
* [[RECONFIGURING]] `RECONFIGURING`

| streamDeathCause
| [[streamDeathCause]] `StreamingQueryException`

| streamMetrics
a| [[streamMetrics]] <<spark-sql-streaming-MetricsReporter.adoc#, MetricsReporter>> with *spark.streaming.[name or id]* source name

Uses <<name, name>> if defined (can be `null`) or falls back to <<id, id>>

| uniqueSources
a| [[uniqueSources]] Unique link:spark-sql-streaming-Source.adoc[streaming data sources] in a streaming Dataset (after being collected as `StreamingExecutionRelation` from the corresponding <<logicalPlan, logical query plan>>).

NOTE: link:spark-sql-streaming-StreamingExecutionRelation.adoc[StreamingExecutionRelation] is a leaf logical operator (i.e. `LogicalPlan`) that represents a streaming data source (and corresponds to a single link:spark-sql-streaming-StreamingRelation.adoc[StreamingRelation] in <<analyzedPlan, analyzed logical query plan>> of a streaming Dataset).

Used when `StreamExecution`:

* <<constructNextBatch, Constructs the next streaming micro-batch>> (and gets new offsets for every streaming data source)

* <<stopSources, Stops all streaming data sources>>
|===

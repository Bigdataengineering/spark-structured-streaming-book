== [[ContinuousExecution]] ContinuousExecution -- StreamExecution in Continuous Stream Processing

`ContinuousExecution` is the <<spark-sql-streaming-StreamExecution.adoc#, StreamExecution>> in <<spark-sql-streaming-continuous-stream-processing.adoc#, Continuous Stream Processing>>.

`ContinuousExecution` is <<creating-instance, created>> when `StreamingQueryManager` is requested to <<spark-sql-streaming-StreamingQueryManager.adoc#createQuery, create a streaming query>> with a <<sink, StreamWriteSupport sink>> and a <<trigger, ContinuousTrigger>> (when `DataStreamWriter` is requested to <<spark-sql-streaming-DataStreamWriter.adoc#start, start an execution of the streaming query>>).

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val sq = spark
  .readStream
  .format("rate")
  .load
  .writeStream
  .format("console")
  .option("truncate", false)
  .trigger(Trigger.Continuous(1.minute)) // <-- Gives ContinuousExecution
  .queryName("rate2console")
  .start

import org.apache.spark.sql.streaming.StreamingQuery
assert(sq.isInstanceOf[StreamingQuery])

// The following gives access to the internals
// And to ContinuousExecution
import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper
val engine = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery
import org.apache.spark.sql.execution.streaming.StreamExecution
assert(engine.isInstanceOf[StreamExecution])

import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution
val continuousEngine = engine.asInstanceOf[ContinuousExecution]
assert(continuousEngine.trigger == Trigger.Continuous(1.minute))
----

`ContinuousExecution` can only run streaming queries with <<spark-sql-streaming-StreamingRelationV2.adoc#, StreamingRelationV2>> with <<spark-sql-streaming-ContinuousReadSupport.adoc#, ContinuousReadSupport>> data source.

When <<creating-instance, created>> (for a streaming query), `ContinuousExecution` is given the <<analyzedPlan, analyzed logical plan>>. The analyzed logical plan is immediately transformed to include a <<spark-sql-streaming-ContinuousExecutionRelation.adoc#, ContinuousExecutionRelation>> for every <<spark-sql-streaming-StreamingRelationV2.adoc#, StreamingRelationV2>> with <<spark-sql-streaming-ContinuousReadSupport.adoc#, ContinuousReadSupport>> data source (and is the <<logicalPlan, logical plan>> internally).

NOTE: `ContinuousExecution` uses the same instance of `ContinuousExecutionRelation` for the same instances of <<spark-sql-streaming-StreamingRelationV2.adoc#, StreamingRelationV2>> with <<spark-sql-streaming-ContinuousReadSupport.adoc#, ContinuousReadSupport>> data source.

`ContinuousExecution` supports exactly one <<continuousSources, ContinuousReader>> in the <<logicalPlan, streaming query>> only (and asserts it when requested to <<addOffset, addOffset>> and <<commit, commit>>).

When requested to <<runContinuous, run the streaming query>>, `ContinuousExecution` collects <<spark-sql-streaming-ContinuousReadSupport.adoc#, ContinuousReadSupport>> data sources (inside <<spark-sql-streaming-ContinuousExecutionRelation.adoc#, ContinuousExecutionRelation>>) from the <<logicalPlan, analyzed logical plan>> and requests each and every `ContinuousReadSupport` to <<spark-sql-streaming-ContinuousReadSupport.adoc#createContinuousReader, create a ContinuousReader>> (that are stored in <<continuousSources, continuousSources>> internal registry).

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution=ALL
```

Refer to <<spark-sql-streaming-logging.adoc#, Logging>>.
====

=== [[getStartOffsets]] `getStartOffsets` Internal Method

[source, scala]
----
getStartOffsets(sparkSessionToRunBatches: SparkSession): OffsetSeq
----

`getStartOffsets`...FIXME

NOTE: `getStartOffsets` is used when...FIXME

=== [[commit]] Committing Epoch -- `commit` Method

[source, scala]
----
commit(epoch: Long): Unit
----

`commit`...FIXME

NOTE: `commit` is used exclusively when `EpochCoordinator` is requested to <<spark-sql-streaming-EpochCoordinator.adoc#commitEpoch, commitEpoch>>.

=== [[awaitEpoch]] `awaitEpoch` Internal Method

[source, scala]
----
awaitEpoch(epoch: Long): Unit
----

`awaitEpoch`...FIXME

NOTE: `awaitEpoch` is used when...FIXME

=== [[addOffset]] `addOffset` Method

[source, scala]
----
addOffset(
  epoch: Long,
  reader: ContinuousReader,
  partitionOffsets: Seq[PartitionOffset]): Unit
----

In essense, `addOffset` requests the given <<spark-sql-streaming-ContinuousReader.adoc#, ContinuousReader>> to <<spark-sql-streaming-ContinuousReader.adoc#mergeOffsets, mergeOffsets>> (with the given `PartitionOffsets`) and then requests the <<spark-sql-streaming-StreamExecution.adoc#offsetLog, OffsetSeqLog>> to <<spark-sql-streaming-HDFSMetadataLog.adoc#add, register the offset with the given epoch>>.

.ContinuousExecution.addOffset
image::images/ContinuousExecution-addOffset.png[align="center"]

Internally, `addOffset` requests the given <<spark-sql-streaming-ContinuousReader.adoc#, ContinuousReader>> to <<spark-sql-streaming-ContinuousReader.adoc#mergeOffsets, mergeOffsets>> (with the given `PartitionOffsets`) and to get the current "global" offset back.

`addOffset` then requests the <<spark-sql-streaming-StreamExecution.adoc#offsetLog, OffsetSeqLog>> to <<spark-sql-streaming-HDFSMetadataLog.adoc#add, add>> the current "global" offset for the given `epoch`.

`addOffset` requests the <<spark-sql-streaming-StreamExecution.adoc#offsetLog, OffsetSeqLog>> for the <<spark-sql-streaming-HDFSMetadataLog.adoc#get, offset at the previous epoch>>.

If the offsets at the current and previous epochs are the same, `addOffset` turns the <<spark-sql-streaming-StreamExecution.adoc#noNewData, noNewData>> internal flag on.

`addOffset` then acquires the <<spark-sql-streaming-StreamExecution.adoc#awaitProgressLock, awaitProgressLock>>, wakes up all threads waiting for the <<spark-sql-streaming-StreamExecution.adoc#awaitProgressLockCondition, awaitProgressLockCondition>> and in the end releases the <<spark-sql-streaming-StreamExecution.adoc#awaitProgressLock, awaitProgressLock>>.

NOTE: `addOffset` supports exactly one <<continuousSources, continuous source>>.

NOTE: `addOffset` is used exclusively when `EpochCoordinator` is requested to <<spark-sql-streaming-EpochCoordinator.adoc#ReportPartitionOffset, handle a ReportPartitionOffset message>>.

=== [[sources]] `sources` Method

[source, scala]
----
sources: Seq[BaseStreamingSource]
----

NOTE: `sources` is part of <<spark-sql-streaming-ProgressReporter.adoc#sources, ProgressReporter Contract>> to...FIXME.

`sources`...FIXME

=== [[logicalPlan]] Analyzed Logical Plan of Streaming Query -- `logicalPlan` Property

[source, scala]
----
logicalPlan: LogicalPlan
----

NOTE: `logicalPlan` is part of <<spark-sql-streaming-StreamExecution.adoc#logicalPlan, StreamExecution Contract>> that is the analyzed logical plan of the streaming query.

`logicalPlan` resolves <<spark-sql-streaming-StreamingRelationV2.adoc#, StreamingRelationV2>> leaf logical operators (with a <<spark-sql-streaming-ContinuousReadSupport.adoc#, ContinuousReadSupport>> source) to <<spark-sql-streaming-ContinuousExecutionRelation.adoc#, ContinuousExecutionRelation>> leaf logical operators.

Internally, `logicalPlan` transforms the <<analyzedPlan, analyzed logical plan>> as follows:

. For every <<spark-sql-streaming-StreamingRelationV2.adoc#, StreamingRelationV2>> leaf logical operator with a <<spark-sql-streaming-ContinuousReadSupport.adoc#, ContinuousReadSupport>> source, `logicalPlan` looks it up for the corresponding <<spark-sql-streaming-ContinuousExecutionRelation.adoc#, ContinuousExecutionRelation>> (if available in the internal lookup registry) or creates a `ContinuousExecutionRelation` (with the `ContinuousReadSupport` source, the options and the output attributes of the `StreamingRelationV2` operator)

. For any other `StreamingRelationV2`, `logicalPlan` throws an `UnsupportedOperationException`:
+
```
Data source [name] does not support continuous processing.
```

=== [[runActivatedStream]] Running Activated Streaming Query -- `runActivatedStream` Method

[source, scala]
----
runActivatedStream(sparkSessionForStream: SparkSession): Unit
----

NOTE: `runActivatedStream` is part of <<spark-sql-streaming-StreamExecution.adoc#runActivatedStream, StreamExecution Contract>> to run a streaming query.

`runActivatedStream`...FIXME

=== [[runContinuous]] Running Streaming Query in Continuous Mode -- `runContinuous` Internal Method

[source, scala]
----
runContinuous(sparkSessionForQuery: SparkSession): Unit
----

`runContinuous`...FIXME

NOTE: `runContinuous` is used exclusively when `ContinuousExecution` is requested to <<runActivatedStream, execute an activated streaming query>>.

=== [[creating-instance]] Creating ContinuousExecution Instance

`ContinuousExecution` takes the following when created:

* [[sparkSession]] `SparkSession`
* [[name]] The name of the structured query
* [[checkpointRoot]] Path to the checkpoint directory (aka _metadata directory_)
* [[analyzedPlan]] Analyzed logical query plan (`LogicalPlan`)
* [[sink]] <<spark-sql-streaming-StreamWriteSupport.adoc#, StreamWriteSupport>>
* [[trigger]] <<spark-sql-streaming-Trigger.adoc#, Trigger>>
* [[triggerClock]] `Clock`
* [[outputMode]] <<spark-sql-streaming-OutputMode.adoc#, Output mode>>
* [[extraOptions]] Options (`Map[String, String]`)
* [[deleteCheckpointOnStop]] `deleteCheckpointOnStop` flag to control whether to delete the checkpoint directory on stop

`ContinuousExecution` initializes the <<internal-registries, internal registries and counters>>.

=== [[stop]] Stopping Streaming Query -- `stop` Method

[source, scala]
----
stop(): Unit
----

NOTE: `stop` is part of the <<spark-sql-streaming-StreamingQuery.adoc#stop, StreamingQuery Contract>> to stop the streaming query.

`stop` transitions the streaming query to `TERMINATED` state.

If the <<spark-sql-streaming-StreamExecution.adoc#queryExecutionThread, queryExecutionThread>> is alive (i.e. it has been started and has not yet died), `stop` interrupts it and waits for this thread to die.

In the end, `stop` prints out the following INFO message to the logs:

```
Query [prettyIdString] was stopped
```

NOTE: <<spark-sql-streaming-StreamExecution.adoc#prettyIdString, prettyIdString>> is in the format of `queryName [id = [id], runId = [runId]]`.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| continuousSources
a| [[continuousSources]]

[source, scala]
----
continuousSources: Seq[ContinuousReader]
----

Registry of <<spark-sql-streaming-ContinuousReader.adoc#, ContinuousReaders>> (in the <<logicalPlan, analyzed logical plan of the streaming query>>)

Used when `ContinuousExecution` is requested to <<commit, commit>>, <<getStartOffsets, getStartOffsets>>, and <<runContinuous, runContinuous>>

Use <<sources, sources>> to access the current value

| currentEpochCoordinatorId
| [[currentEpochCoordinatorId]] FIXME

Used when...FIXME

| triggerExecutor
a| [[triggerExecutor]] <<spark-sql-streaming-TriggerExecutor.adoc#, TriggerExecutor>> for the <<trigger, Trigger>>:

* `ProcessingTimeExecutor` for `ContinuousTrigger`

Used when...FIXME

NOTE: `StreamExecution` throws an `IllegalStateException` when the <<trigger, Trigger>> is not a <<spark-sql-streaming-Trigger.adoc#ContinuousTrigger, ContinuousTrigger>>.
|===

== Kafka Data Source -- Streaming Data Source for Apache Kafka

*Kafka Data Source* is a streaming data source for https://kafka.apache.org/[Apache Kafka].

[source, scala]
----
val records = spark
  .readStream
  .format("kafka") // via KafkaSourceProvider
  .option("subscribepattern", """topic\d""") // topics with a digit at the end
  .option("kafka.bootstrap.servers", ":9092")
  .option("startingOffsets", "latest")
  .option("maxOffsetsPerTrigger", 1)
  .load

assert(records.isStreaming)

scala> records.printSchema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

val values = records
  .select($"value" cast "string") // deserialize values

import org.apache.spark.sql.streaming.{OutputMode, Trigger}
import scala.concurrent.duration._
val sq = values
  .writeStream
  .format("console")
  .option("truncate", false)
  .queryName("kafka-to-console")
  .trigger(Trigger.ProcessingTime(30.seconds)) // micro-batch stream processing
  .outputMode(OutputMode.Append)
  .start
scala> sq.explain
== Physical Plan ==
WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@2575b2b3
+- *(1) Project [cast(value#25 as string) AS value#21]
   +- *(1) Project [key#24, value#25, topic#26, partition#27, offset#28L, timestamp#29, timestampType#30]
      +- *(1) ScanV2 kafka[key#24, value#25, topic#26, partition#27, offset#28L, timestamp#29, timestampType#30] (Options: [subscribe=t1,kafka.bootstrap.servers=:9092])

// In the end, stop the streaming query
sq.stop
----

=== kafka Data Source Format

Kafka Data Source is registered under *kafka* data source format (via <<spark-sql-streaming-KafkaSourceProvider.adoc#, KafkaSourceProvider>> and the corresponding `META-INF/services/org.apache.spark.sql.sources.DataSourceRegister` service registration file).

[source, scala]
----
val inputs = spark
  .readStream
  .format("kafka") // <-- uses Kafka data source
  .option("subscribepattern", "input.*")
  .option("kafka.bootstrap.servers", ":9092")
  .load
scala> inputs.explain(extended = true)
== Parsed Logical Plan ==
StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2300d2b4, kafka, Map(subscribepattern -> input.*, kafka.bootstrap.servers -> :9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@30090808,kafka,List(),None,List(),None,Map(subscribepattern -> input.*, kafka.bootstrap.servers -> :9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
...
== Physical Plan ==
StreamingRelation kafka, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]
----

NOTE: <<spark-sql-streaming-KafkaSourceProvider.adoc#, KafkaSourceProvider>> is the provider for streaming <<spark-sql-streaming-StreamSourceProvider.adoc#, sources>> and <<spark-sql-streaming-StreamSinkProvider.adoc#, sinks>> for the Kafka data source.

=== [[spark-sql-kafka-0-10]] spark-sql-kafka-0-10 External Module

Kafka Data Source is part of the *spark-sql-kafka-0-10* external module that is distributed with the official distribution of Apache Spark, but it is not included in the CLASSPATH by default.

Define `spark-sql-kafka-0-10` as part of a build definition (e.g. as a `libraryDependency` in `build.sbt` for sbt) as follows:

```
libraryDependencies += "org.apache.spark" %% "spark-sql-kafka-0-10" % "{{ book.version }}"
```

Use `--packages` command-line option of `spark-submit` shell script (and "derivatives" like `spark-shell`) while submitting a Spark Structured Streaming application for execution.

```
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:{{ book.version }}
```

NOTE: Replace the version of `spark-sql-kafka-0-10` module (e.g. `{{ book.version }}` above) with one of the available versions found at https://search.maven.org/search?q=a:spark-sql-kafka-0-10_2.12[The Central Repository's Search] that matches your version of Apache Spark.

=== [[schema]] Predefined (Fixed) Schema

Kafka Data Source uses a predefined (fixed) schema.

.Kafka Data Source's Fixed Schema (in the positional order)
[cols="1m,2m",options="header",width="100%"]
|===
| Name
| Type

| key
| BinaryType

| value
| BinaryType

| topic
| StringType

| partition
| IntegerType

| offset
| LongType

| timestamp
| TimestampType

| timestampType
| IntegerType

|===

[TIP]
====
Use `Column.cast` operator to cast `BinaryType` to a `StringType` (for `key` and `value` columns).

[source, scala]
----
scala> :type records
org.apache.spark.sql.DataFrame

val values = records.select($"value" cast "string")
scala> values.printSchema
root
 |-- value: string (nullable = true)
----

====

=== Micro-Batch Stream Processing

Kafka Data Source supports <<spark-sql-streaming-micro-batch-processing.adoc#, Micro-Batch Stream Processing>> (i.e. <<spark-sql-streaming-Trigger.adoc#Once, Trigger.Once>> and <<spark-sql-streaming-Trigger.adoc#ProcessingTime, Trigger.ProcessingTime>> triggers) via <<spark-sql-streaming-KafkaMicroBatchReader.adoc#, KafkaMicroBatchReader>>.

Kafka Data Source can assign a single task per Kafka partition (using <<spark-sql-streaming-KafkaOffsetRangeCalculator.adoc#, KafkaOffsetRangeCalculator>> in <<spark-sql-streaming-micro-batch-processing.adoc#, Micro-Batch Stream Processing>>).

Kafka Data Source can reuse a Kafka consumer (using <<spark-sql-streaming-KafkaMicroBatchReader.adoc#, KafkaMicroBatchReader>> in <<spark-sql-streaming-micro-batch-processing.adoc#, Micro-Batch Stream Processing>>).

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val sq = spark
  .readStream
  .format("kafka")
  .option("subscribepattern", "kafka2console.*")
  .option("kafka.bootstrap.servers", ":9092")
  .option("failOnDataLoss", false) // for demo purposes. Don't use in production
  .load
  .withColumn("value", $"value" cast "string") // convert bytes to string for display purposes
  .writeStream
  .format("console")
  .option("truncate", false) // format-specific option
  .queryName("kafka2console-microbatch")
  .trigger(Trigger.ProcessingTime(30.seconds))
  .option("checkpointLocation", "checkpointLocation-kafka2console") // generic query option
  .start
----

=== Continuous Stream Processing

Kafka Data Source supports <<spark-sql-streaming-continuous-stream-processing.adoc#, Continuous Stream Processing>> (i.e. <<spark-sql-streaming-Trigger.adoc#Continuous, Trigger.Continuous>> trigger) via <<spark-sql-streaming-KafkaContinuousReader.adoc#, KafkaContinuousReader>>.

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val sq = spark
  .readStream
  .format("kafka")
  .option("subscribepattern", "kafka2console.*")
  .option("kafka.bootstrap.servers", ":9092")
  .option("failOnDataLoss", false) // for demo purposes. Don't use in production
  .load
  .withColumn("value", $"value" cast "string") // convert bytes to string for display purposes
  .writeStream
  .format("console")
  .option("truncate", false) // format-specific option
  .queryName("kafka2console-continuous")
  .trigger(Trigger.Continuous(10.seconds))
  .option("checkpointLocation", "checkpointLocation-kafka2console") // generic query option
  .start
----

=== [[options]] Configuration Options

NOTE: Options with *kafka.* prefix (e.g. `kafka.bootstrap.servers`) are considered configuration properties for Kafka consumers used on the <<spark-sql-streaming-KafkaSourceProvider.adoc#kafkaParamsForDriver, driver>> and <<spark-sql-streaming-KafkaSourceProvider.adoc#kafkaParamsForExecutors, executors>>.

.Kafka Data Source's Options (Case-Insensitive)
[cols="1m,3",options="header",width="100%"]
|===
| Option
| Description

| assign
a| [[assign]] link:spark-sql-streaming-ConsumerStrategy.adoc#AssignStrategy[Topic subscription strategy] that accepts a JSON with topic names and partitions, e.g.

```
{"topicA":[0,1],"topicB":[0,1]}
```

NOTE: Exactly one topic subscription strategy is allowed (that `KafkaSourceProvider` link:spark-sql-streaming-KafkaSourceProvider.adoc#validateGeneralOptions[validates] before creating `KafkaSource`).

| failOnDataLoss
a| [[failOnDataLoss]] Flag to control whether...FIXME

Default: `true`

Used when `KafkaSourceProvider` is requested for <<spark-sql-streaming-KafkaSourceProvider.adoc#failOnDataLoss, failOnDataLoss configuration property>>

| kafkaConsumer.pollTimeoutMs
a| [[kafkaConsumer.pollTimeoutMs]][[pollTimeoutMs]] The time (in milliseconds) spent waiting in `Consumer.poll` if data is not available in the buffer.

Default: `spark.network.timeout` or `120s`

Used when...FIXME

| maxOffsetsPerTrigger
a| [[maxOffsetsPerTrigger]] Number of records to fetch per trigger (to limit the number of records to fetch).

Default: `(undefined)`

Unless defined, `KafkaSource` requests <<spark-sql-streaming-KafkaSource.adoc#kafkaReader, KafkaOffsetReader>> for the link:spark-sql-streaming-KafkaOffsetReader.adoc#fetchLatestOffsets[latest offsets].

| minPartitions
a| [[minPartitions]] Minimum number of partitions per executor (given Kafka partitions)

Default: `(undefined)`

Must be undefined (default) or greater than `0`

When undefined (default) or smaller than the number of `TopicPartitions` with records to consume from, <<spark-sql-streaming-KafkaMicroBatchReader.adoc#, KafkaMicroBatchReader>> uses <<spark-sql-streaming-KafkaMicroBatchReader.adoc#rangeCalculator, KafkaOffsetRangeCalculator>> to <<spark-sql-streaming-KafkaOffsetRangeCalculator.adoc#getLocation, find the preferred executor>> for every `TopicPartition` (and the <<spark-sql-streaming-KafkaMicroBatchReader.adoc#getSortedExecutorList, available executors>>).

| startingOffsets
a| [[startingOffsets]] Starting offsets

Possible values:

* `latest`

* `earliest`

* JSON with topics, partitions and their starting offsets, e.g.
+
```
{"topicA":{"part":offset,"p1":-1},"topicB":{"0":-2}}
```

[TIP]
====
Use Scala's tripple quotes for the JSON for topics, partitions and offsets.

[source, scala]
----
option(
  "startingOffsets",
  """{"topic1":{"0":5,"4":-1},"topic2":{"0":-2}}""")
----
====

| subscribe
a| [[subscribe]] link:spark-sql-streaming-ConsumerStrategy.adoc#SubscribeStrategy[Topic subscription strategy] that accepts topic names as a comma-separated string, e.g.

```
topic1,topic2,topic3
```

NOTE: Exactly one topic subscription strategy is allowed (that `KafkaSourceProvider` link:spark-sql-streaming-KafkaSourceProvider.adoc#validateGeneralOptions[validates] before creating `KafkaSource`).

| subscribepattern
a| [[subscribepattern]] link:spark-sql-streaming-ConsumerStrategy.adoc#SubscribePatternStrategy[Topic subscription strategy] that uses Java's http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern] for the topic subscription regex pattern of topics to subscribe to, e.g.

```
topic\d
```

[TIP]
====
Use Scala's tripple quotes for the regular expression for topic subscription regex pattern.

[source, scala]
----
option("subscribepattern", """topic\d""")
----
====

NOTE: Exactly one topic subscription strategy is allowed (that `KafkaSourceProvider` link:spark-sql-streaming-KafkaSourceProvider.adoc#validateGeneralOptions[validates] before creating `KafkaSource`).

|===

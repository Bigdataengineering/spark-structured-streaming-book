== [[HDFSBackedStateStore]] HDFSBackedStateStore -- State Store on HDFS-Compatible File System

`HDFSBackedStateStore` is a concrete <<spark-sql-streaming-StateStore.adoc#, StateStore>> that uses a Hadoop DFS-compatible file system for versioned state persistence.

`HDFSBackedStateStore` is <<creating-instance, created>> exclusively when `HDFSBackedStateStoreProvider` is requested for a <<getStore, state store for a given version>> (when `StateStore` utility is requested to <<spark-sql-streaming-StateStore.adoc#get-StateStore, retrieve the StateStore for a given ID and version>>).

`HDFSBackedStateStore` can be in the following <<state, states>>:

* `UPDATING`
* `COMMITTED`
* `ABORTED`

[[id]]
`HDFSBackedStateStore` uses the <<spark-sql-streaming-StateStoreId.adoc#, StateStoreId>> of the owning <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#stateStoreId, HDFSBackedStateStoreProvider>>.

[[toString]]
When requested for the textual representation, `HDFSBackedStateStore` gives *HDFSStateStore[id=(op=[operatorId],part=[partitionId]),dir=[baseDir]]*.

[[logging]]
[TIP]
====
`HDFSBackedStateStore` is an internal class of <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#, HDFSBackedStateStoreProvider>> and uses its <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#logging, logger>>.
====

=== [[creating-instance]] Creating HDFSBackedStateStore Instance

`HDFSBackedStateStore` takes the following to be created:

* [[version]] Version
* [[mapToUpdate]] State (`ConcurrentHashMap[UnsafeRow, UnsafeRow]`)

`HDFSBackedStateStore` initializes the <<internal-properties, internal properties>>.

=== [[writeUpdateToDeltaFile]] `writeUpdateToDeltaFile` Internal Method

[source, scala]
----
writeUpdateToDeltaFile(
  output: DataOutputStream,
  key: UnsafeRow,
  value: UnsafeRow): Unit
----

CAUTION: FIXME

=== [[put]] `put` Method

[source, scala]
----
put(key: UnsafeRow, value: UnsafeRow): Unit
----

NOTE: `put` is a part of link:spark-sql-streaming-StateStore.adoc#put[StateStore Contract] to...FIXME

`put` stores the copies of the key and value in <<mapToUpdate, mapToUpdate>> internal registry followed by <<writeUpdateToDeltaFile, writing them to a delta file>> (using <<tempDeltaFileStream, tempDeltaFileStream>>).

[NOTE]
====
`put` can only be used when `HDFSBackedStateStore` is in `UPDATING` state and reports a `IllegalStateException` otherwise.

```
Cannot put after already committed or aborted
```
====

=== [[commit]] Committing State Changes -- `commit` Method

[source, scala]
----
commit(): Long
----

NOTE: `commit` is part of the <<spark-sql-streaming-StateStore.adoc#commit, StateStore Contract>> to commit state changes.

`commit` requests the parent `HDFSBackedStateStoreProvider` to <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#commitUpdates, commit state changes (as a new version of state)>> (with the <<newVersion, newVersion>>, the <<mapToUpdate, mapToUpdate>> and the <<compressedStream, compressed stream>>).

`commit` sets the state to `COMMITTED`.

`commit` prints out the following INFO message to the logs:

```
Committed version [newVersion] for [this] to file [finalDeltaFile]
```

`commit` returns a <<newVersion, newVersion>>.

`commit` throws a `IllegalStateException` when executed in any state but `UPDATING` state:

```
Cannot commit after already committed or aborted
```

`commit` throws a `IllegalStateException` for any `NonFatal` exception:

```
Error committing version [newVersion] into [this]
```

=== [[abort]] Aborting State Changes -- `abort` Method

[source, scala]
----
abort(): Unit
----

NOTE: `abort` is part of the <<spark-sql-streaming-StateStore.adoc#abort, StateStore Contract>> to abort the state changes.

`abort`...FIXME

=== [[metrics]] Performance Metrics -- `metrics` Method

[source, scala]
----
metrics: StateStoreMetrics
----

NOTE: `metrics` is part of the <<spark-sql-streaming-StateStore.adoc#metrics, StateStore Contract>> to get the <<spark-sql-streaming-StateStoreMetrics.adoc#, StateStoreMetrics>>.

`metrics` requests the <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#getMetricsForProvider, performance metrics>> of the parent `HDFSBackedStateStoreProvider`.

The performance metrics of the provider used are only the ones listed in <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#supportedCustomMetrics, supportedCustomMetrics>>.

In the end, `metrics` returns a new <<spark-sql-streaming-StateStoreMetrics.adoc#, StateStoreMetrics>> with the following:

* <<spark-sql-streaming-StateStoreMetrics.adoc#numKeys, Total number of keys>> as the size of <<mapToUpdate, mapToUpdate>>

* <<spark-sql-streaming-StateStoreMetrics.adoc#memoryUsedBytes, Memory used (in bytes)>> as the <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#memoryUsedBytes, memoryUsedBytes>> metric (of the parent provider)

* <<spark-sql-streaming-StateStoreMetrics.adoc#customMetrics, StateStoreCustomMetrics>> as the <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#supportedCustomMetrics, supportedCustomMetrics>> and the <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#metricStateOnCurrentVersionSizeBytes, metricStateOnCurrentVersionSizeBytes>> metric of the parent provider

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| compressedStream
a| [[compressedStream]]

[source, scala]
----
compressedStream: DataOutputStream
----

The compressed https://docs.oracle.com/javase/8/docs/api/java/io/DataOutputStream.html[java.io.DataOutputStream] for the <<deltaFileStream, deltaFileStream>>

| deltaFileStream
a| [[deltaFileStream]]

[source, scala]
----
deltaFileStream: CheckpointFileManager.CancellableFSDataOutputStream
----

| finalDeltaFile
a| [[finalDeltaFile]]

[source, scala]
----
finalDeltaFile: Path
----

The Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] of the <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#deltaFile, deltaFile>> for the <<newVersion, version>>

| newVersion
a| [[newVersion]]

[source, scala]
----
newVersion: Long
----

Used exclusively when `HDFSBackedStateStore` is requested for the <<finalDeltaFile, finalDeltaFile>>, to <<commit, commit>> and <<abort, abort>>

| state
a| [[state]]

[source, scala]
----
state: STATE
----

|===

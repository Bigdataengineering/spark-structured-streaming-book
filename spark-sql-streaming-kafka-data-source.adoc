== Kafka Data Source -- Streaming Data Source for Apache Kafka

*Kafka Data Source* is a streaming data source for https://kafka.apache.org/[Apache Kafka].

[source, scala]
----
val records = spark
  .readStream
  .format("kafka")
  .option("subscribe", "t1")
  .option("kafka.bootstrap.servers", ":9092")
  .load

assert(records.isStreaming)

scala> records.printSchema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

val values = records.select($"value" cast "string")
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val sq = values
  .writeStream
  .format("console")
  .option("truncate", false)
  .queryName("kafka-to-console")
  .trigger(Trigger.ProcessingTime(30.seconds))
  .start
scala> sq.explain
== Physical Plan ==
WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@2575b2b3
+- *(1) Project [cast(value#25 as string) AS value#21]
   +- *(1) Project [key#24, value#25, topic#26, partition#27, offset#28L, timestamp#29, timestampType#30]
      +- *(1) ScanV2 kafka[key#24, value#25, topic#26, partition#27, offset#28L, timestamp#29, timestampType#30] (Options: [subscribe=t1,kafka.bootstrap.servers=:9092])
----

=== [[spark-sql-kafka-0-10]] spark-sql-kafka-0-10 Library Dependency

The structured streaming API for Kafka is part of the *spark-sql-kafka-0-10* external project. Although the `spark-sql-kafka-0-10` external project is indeed part of the official distribution of Apache Spark, but it is not included in the CLASSPATH by default.

You should add the following dependency to sbt project to use the Kafka data source:

```
libraryDependencies += "org.apache.spark" %% "spark-sql-kafka-0-10" % "{{ book.version }}"
```

[TIP]
====
`spark-sql-kafka-0-10` module is not included in the CLASSPATH of `spark-shell` so you have to start it with `--packages` command-line option.

```
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:{{ book.version }}
```
====

NOTE: Replace `{{ book.version }}` with one of the available versions found at http://search.maven.org/#search%7Cga%7C1%7Ca%3A%22spark-sql-kafka-0-10_2.12%22[The Central Repository's Search] that matches your version of Spark.

=== [[schema]] Predefined (Fixed) Schema

Kafka Data Source uses a predefined (fixed) schema.

.Kafka Data Source's Fixed Schema (in the positional order)
[cols="1m,2m",options="header",width="100%"]
|===
| Name
| Type

| key
| BinaryType

| value
| BinaryType

| topic
| StringType

| partition
| IntegerType

| offset
| LongType

| timestamp
| TimestampType

| timestampType
| IntegerType

|===

[TIP]
====
Use `Column.cast` operator to cast `BinaryType` to a `StringType` (for `key` and `value` columns).

[source, scala]
----
scala> :type records
org.apache.spark.sql.DataFrame

val values = records.select($"value" cast "string")
scala> values.printSchema
root
 |-- value: string (nullable = true)
----

====

=== External Module

Kafka Data Source is an external module that is distributed as part of the official distribution of Apache Spark, but is not available on the CLASSPATH by default. You should define it as part of a build definition (e.g. as a `libraryDependency` in `build.sbt` for sbt) or use `--packages` command-line option when `spark-submit` a Spark Structured Streaming application.

All the examples that follow assume that you started `spark-shell` with the `--packages` command-line option.

[source, scala]
----
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:{{ book.version }}
----

=== kafka Format

Kafka Data Source is registered under *kafka* data source format (via <<spark-sql-streaming-KafkaSourceProvider.adoc#, KafkaSourceProvider>> and the corresponding `META-INF/services/org.apache.spark.sql.sources.DataSourceRegister` service registration file).

[source, scala]
----
val inputs = spark
  .readStream
  .format("kafka") // <-- uses Kafka data source
  .option("subscribepattern", "input*")
  .option("kafka.bootstrap.servers", ":9092")
  .load
scala> inputs.explain(extended = true)
== Parsed Logical Plan ==
StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1acb7cf0, kafka, Map(subscribepattern -> input*, kafka.bootstrap.servers -> :9092), [key#4344, value#4345, topic#4346, partition#4347, offset#4348L, timestamp#4349, timestampType#4350], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1b33bbbe,kafka,List(),None,List(),None,Map(subscribepattern -> input*, kafka.bootstrap.servers -> :9092),None), kafka, [key#4337, value#4338, topic#4339, partition#4340, offset#4341L, timestamp#4342, timestampType#4343]
...
----

NOTE: <<spark-sql-streaming-KafkaSourceProvider.adoc#, KafkaSourceProvider>> is the provider for streaming <<spark-sql-streaming-StreamSourceProvider.adoc#, sources>> and <<spark-sql-streaming-StreamSinkProvider.adoc#, sinks>> for the Kafka data source.

=== Continuous Stream Processing

Kafka Data Source supports <<spark-sql-streaming-continuous-stream-processing.adoc#, Continuous Stream Processing>> (i.e. <<spark-sql-streaming-Trigger.adoc#Continuous, Trigger.Continuous>> trigger) via <<spark-sql-streaming-KafkaContinuousReader.adoc#, KafkaContinuousReader>>.

[source, scala]
----
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val sq = spark
  .readStream
  .format("kafka")
  .option("subscribepattern", "kafka2console.*")
  .option("kafka.bootstrap.servers", ":9092")
  .option("failOnDataLoss", false) // <-- for demo purposes. Don't use in production
  .load
  .withColumn("value", $"value" cast "string") // <-- convert bytes to string for display purposes
  .writeStream
  .trigger(Trigger.Continuous(10.seconds))
  // query-generic option
  .option("checkpointLocation", "checkpointLocation-kafka2console")
  .format("console")
  // format-specific option
  .option("truncate", false)
  .queryName("kafka2console")
  .start
----

=== Micro-Batch Stream Processing

Kafka Data Source supports <<spark-sql-streaming-micro-batch-processing.adoc#, Micro-Batch Stream Processing>> (i.e. <<spark-sql-streaming-Trigger.adoc#Once, Trigger.Once>> and <<spark-sql-streaming-Trigger.adoc#ProcessingTime, Trigger.ProcessingTime>> triggers) via <<spark-sql-streaming-KafkaMicroBatchReader.adoc#, KafkaMicroBatchReader>>.

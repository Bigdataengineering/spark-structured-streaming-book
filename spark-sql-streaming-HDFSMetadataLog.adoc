== [[HDFSMetadataLog]] HDFSMetadataLog -- MetadataLog with Hadoop HDFS for Reliable Storage

`HDFSMetadataLog` is a concrete <<spark-sql-streaming-MetadataLog.adoc#, MetadataLog>> that uses Hadoop HDFS for a reliable storage.

[[metadataPath]]
`HDFSMetadataLog` uses the given <<path, path>> as the root directory of metadata logs. The path is immediately converted to a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] for file management.

[[formats]]
`HDFSMetadataLog` uses http://json4s.org/[Json4s] with the https://github.com/FasterXML/jackson-databind[Jackson] binding for JSON parsing (serialization and deserialization).

`HDFSMetadataLog` is further customized by the <<extensions, extensions>>.

[[extensions]]
.HDFSMetadataLogs
[cols="30,70",options="header",width="100%"]
|===
| HDFSMetadataLog
| Description

| <<spark-sql-streaming-CommitLog.adoc#, CommitLog>>
| [[CommitLog]] <<spark-sql-streaming-StreamExecution.adoc#commitLog, Offset commit log>> of <<spark-sql-streaming-StreamExecution.adoc#, streaming query execution engines>>

| <<spark-sql-streaming-CompactibleFileStreamLog.adoc#, CompactibleFileStreamLog>>
| [[CompactibleFileStreamLog]] Compactible metadata logs

| <<spark-sql-streaming-OffsetSeqLog.adoc#, OffsetSeqLog>>
| [[OffsetSeqLog]] <<spark-sql-streaming-StreamExecution.adoc#offsetLog, Write-ahead log (WAL)>> of <<spark-sql-streaming-StreamExecution.adoc#, streaming query execution engines>>

|===

[[creating-instance]]
`HDFSMetadataLog` takes the following to be created:

* [[sparkSession]] `SparkSession`
* [[path]] Path of the metadata log directory

While being <<creating-instance, created>> `HDFSMetadataLog` creates the <<path, path>> unless exists already.

=== [[serialize]] Writing Metadata in Serialized Format -- `serialize` Method

[source, scala]
----
serialize(metadata: T, out: OutputStream): Unit
----

`serialize`...FIXME

NOTE: `serialize` is used exclusively when `HDFSMetadataLog` is requested to <<writeBatchToFile, writeBatchToFile>> (when requested to <<add, store metadata for a batch>>).

=== [[deserialize]] Deserializing Metadata -- `deserialize` Method

[source, scala]
----
deserialize(in: InputStream): T
----

`deserialize` deserializes a metadata (of type `T`) from a given `InputStream`.

NOTE: `deserialize` is used exclusively when `HDFSMetadataLog` is requested to <<get, retrieve metadata for a batch>>.

=== [[createFileManager]] `createFileManager` Internal Method

[source, scala]
----
createFileManager(): FileManager
----

CAUTION: FIXME

NOTE: `createFileManager` is used exclusively when `HDFSMetadataLog` is <<creating-instance, created>> (and the internal <<fileManager, FileManager>> is created alongside).

=== [[get]][[get-batchId]] Retrieving Metadata For Batch -- `get` Method

[source, scala]
----
get(batchId: Long): Option[T]
----

NOTE: `get` is part of the <<spark-sql-streaming-MetadataLog.adoc#get, MetadataLog Contract>> to...FIXME.

`get`...FIXME

=== [[get-range]] Retrieving Metadata For Batch Id Range -- `get` Method

[source, scala]
----
get(
  startId: Option[Long],
  endId: Option[Long]): Array[(Long, T)]
----

NOTE: `get` is part of the <<spark-sql-streaming-MetadataLog.adoc#get, MetadataLog Contract>> to...FIXME.

`get`...FIXME

=== [[add]] Adding Metadata for Batch -- `add` Method

[source, scala]
----
add(
  batchId: Long,
  metadata: T): Boolean
----

NOTE: `add` is part of the <<spark-sql-streaming-MetadataLog.adoc#add, MetadataLog Contract>> to add metadata for a batch.

`add` <<batchIdToPath, creates a batch metadata file>> for the given `batchId` and <<writeBatchToFile, writes the batch metadata to the file>> unless <<get, already done>>.

`add` returns `true` when the metadata has just been written for the batch (and the <<batchIdToPath, batchIdToPath>> for the given `batchId` was not available).

=== [[getLatest]] Retrieving Latest Committed Batch Id with Metadata If Available -- `getLatest` Method

[source, scala]
----
getLatest(): Option[(Long, T)]
----

NOTE: `getLatest` is a part of link:spark-sql-streaming-MetadataLog.adoc#getLatest[MetadataLog Contract] to retrieve the recently-committed batch id and the corresponding metadata if available in the metadata storage.

`getLatest` requests the internal <<fileManager, FileManager>> for the files in <<metadataPath, metadata directory>> that match <<batchFilesFilter, batch file filter>>.

`getLatest` takes the batch ids (the batch files correspond to) and sorts the ids in reverse order.

`getLatest` gives the first batch id with the metadata which <<get, could be found in the metadata storage>>.

NOTE: It is possible that the batch id could be in the metadata storage, but not available for retrieval.

=== [[purge]] Removing Expired Metadata (Purging) -- `purge` Method

[source, scala]
----
purge(thresholdBatchId: Long): Unit
----

NOTE: `purge` is part of the <<spark-sql-streaming-MetadataLog.adoc#purge, MetadataLog Contract>> to...FIXME.

`purge`...FIXME

=== [[getOrderedBatchFiles]] `getOrderedBatchFiles` Method

[source, scala]
----
getOrderedBatchFiles(): Array[FileStatus]
----

`getOrderedBatchFiles`...FIXME

NOTE: `getOrderedBatchFiles` is used when...FIXME

=== [[batchIdToPath]] Creating Batch Metadata File -- `batchIdToPath` Method

[source, scala]
----
batchIdToPath(batchId: Long): Path
----

`batchIdToPath` simply creates a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] for the file called by the specified `batchId` under the <<metadataPath, metadata directory>>.

NOTE: `batchIdToPath` is used when...FIXME

=== [[isBatchFile]] `isBatchFile` Method

[source, scala]
----
isBatchFile(path: Path): Boolean
----

`isBatchFile`...FIXME

NOTE: `isBatchFile` is used when...FIXME

=== [[parseVersion]] Retrieving Version (From Text Line) -- `parseVersion` Internal Method

[source, scala]
----
parseVersion(text: String, maxSupportedVersion: Int): Int
----

`parseVersion`...FIXME

NOTE: `parseVersion` is used when...FIXME

=== [[pathToBatchId]] `pathToBatchId` Method

[source, scala]
----
pathToBatchId(path: Path): Long
----

`pathToBatchId`...FIXME

NOTE: `pathToBatchId` is used when...FIXME

=== [[writeBatchToFile]] Writing Batch Metadata to File -- `writeBatchToFile` Internal Method

[source, scala]
----
writeBatchToFile(
  metadata: T,
  path: Path): Unit
----

`writeBatchToFile` requests the <<fileManager, CheckpointFileManager>> to <<spark-sql-streaming-CheckpointFileManager.adoc#createAtomic, createAtomic>> (for the specified `path` and the `overwriteIfPossible` flag disabled).

`writeBatchToFile` then <<serialize, serializes the metadata>> (to the `CancellableFSDataOutputStream` output stream) and closes the stream.

In case of an exception, `writeBatchToFile` simply requests the `CancellableFSDataOutputStream` output stream to `cancel` (so that the output file is not generated) and re-throws the exception.

NOTE: `writeBatchToFile` is used exclusively when `HDFSMetadataLog` is requested to <<add, add (persists) metadata for a batch>>.

=== [[purgeAfter]] `purgeAfter` Method

[source, scala]
----
purgeAfter(thresholdBatchId: Long): Unit
----

`purgeAfter`...FIXME

NOTE: `purgeAfter` seems to be used exclusively in tests.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| batchFilesFilter
a| [[batchFilesFilter]] Hadoop HDFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/PathFilter.html[PathFilter] of <<isBatchFile, batch files>> (with names being long numbers)

Used when:

* `CompactibleFileStreamLog` is requested for the <<spark-sql-streaming-CompactibleFileStreamLog.adoc#compactInterval, compactInterval>>

* `HDFSMetadataLog` is requested to <<get, get batch metadata>>, <<getLatest, getLatest>>, <<getOrderedBatchFiles, getOrderedBatchFiles>>, <<purge, purge>>, and <<purgeAfter, purgeAfter>>

| fileManager
a| [[fileManager]] `CheckpointFileManager`

Used when...FIXME

|===

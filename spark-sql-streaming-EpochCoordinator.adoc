== [[EpochCoordinator]] EpochCoordinator RPC Endpoint

`EpochCoordinator` is a `ThreadSafeRpcEndpoint` that receive <<messages, messages>> from...FIXME

`EpochCoordinator` is <<creating-instance, created>> (using <<create, create>> factory method) when `ContinuousExecution` is requested to <<spark-sql-streaming-ContinuousExecution.adoc#runContinuous, run a streaming query in continuous mode>>.

[[messages]]
[[EpochCoordinatorMessage]]
.EpochCoordinator RPC Endpoint's Messages
[cols="30m,70",options="header",width="100%"]
|===
| Message
| Description

| CommitPartitionEpoch
| [[CommitPartitionEpoch]]

| GetCurrentEpoch
| [[GetCurrentEpoch]]

| IncrementAndGetEpoch
| [[IncrementAndGetEpoch]]

a| `ReportPartitionOffset`

* [[ReportPartitionOffset-partitionId]] `partitionId`
* [[ReportPartitionOffset-epoch]] `epoch`
* [[ReportPartitionOffset-offset]] `PartitionOffset`

| [[ReportPartitionOffset]] Sent out exclusively when `ContinuousQueuedDataReader` is requested for the <<spark-sql-streaming-ContinuousQueuedDataReader.adoc#next, next row>> to be read in the current epoch, and the epoch is done

| SetReaderPartitions
| [[SetReaderPartitions]]

| SetWriterPartitions
| [[SetWriterPartitions]]

a| `StopContinuousExecutionWrites`
| [[StopContinuousExecutionWrites]] Send out exclusively when `ContinuousExecution` is requested to <<spark-sql-streaming-ContinuousExecution.adoc#runContinuous, run a streaming query in continuous mode>> (and it finishes successfully or not)

|===

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef*` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef*=ALL
```

Refer to <<spark-sql-streaming-logging.adoc#, Logging>>.
====

=== [[receive]] Receiving Messages -- `receive` Method

[source, scala]
----
receive: PartialFunction[Any, Unit]
----

NOTE: `receive` is part of the `RpcEndpoint` Contract in Apache Spark to receive messages.

`receive`...FIXME

==== [[resolveCommitsAtEpoch]] `resolveCommitsAtEpoch` Internal Method

[source, scala]
----
resolveCommitsAtEpoch(epoch: Long): Unit
----

`resolveCommitsAtEpoch`...FIXME

NOTE: `resolveCommitsAtEpoch` is used exclusively when `EpochCoordinator` is requested to <<receive, receive>> `CommitPartitionEpoch` and <<ReportPartitionOffset, ReportPartitionOffset>> messages.

==== [[commitEpoch]] `commitEpoch` Internal Method

[source, scala]
----
commitEpoch(
  epoch: Long,
  messages: Iterable[WriterCommitMessage]): Unit
----

`commitEpoch`...FIXME

NOTE: `commitEpoch` is used exclusively when `EpochCoordinator` is requested to <<resolveCommitsAtEpoch, resolveCommitsAtEpoch>>.

=== [[creating-instance]] Creating EpochCoordinator Instance

`EpochCoordinator` takes the following to be created:

* [[writer]] <<spark-sql-streaming-StreamWriter.adoc#, StreamWriter>>
* [[reader]] <<spark-sql-streaming-ContinuousReader.adoc#, ContinuousReader>>
* [[query]] <<spark-sql-streaming-ContinuousExecution.adoc#, ContinuousExecution>>
* [[startEpoch]] Start epoch
* [[session]] `SparkSession`
* [[rpcEnv]] `RpcEnv`

`EpochCoordinator` initializes the <<internal-properties, internal properties>>.

=== [[create]] Registering EpochCoordinator RPC Endpoint -- `create` Factory Method

[source, scala]
----
create(
  writer: StreamWriter,
  reader: ContinuousReader,
  query: ContinuousExecution,
  epochCoordinatorId: String,
  startEpoch: Long,
  session: SparkSession,
  env: SparkEnv): RpcEndpointRef
----

`create` simply <<creating-instance, creates a new EpochCoordinator>> and requests the `RpcEnv` to register a RPC endpoint as *EpochCoordinator-[id]* (where `id` is the given `epochCoordinatorId`).

`create` prints out the following INFO message to the logs:

```
Registered EpochCoordinator endpoint
```

NOTE: `create` is used exclusively when `ContinuousExecution` is requested to <<spark-sql-streaming-ContinuousExecution.adoc#runContinuous, run a streaming query in continuous mode>>.

=== [[receiveAndReply]] Receiving Messages (Synchronous Mode) -- `receiveAndReply` Method

[source, scala]
----
receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit]
----

NOTE: `receiveAndReply` is part of the `RpcEndpoint` Contract in Apache Spark to receive and reply to messages.

`receiveAndReply`...FIXME

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| queryWritesStopped
| [[queryWritesStopped]] Flag that indicates whether to drop messages (`true`) or not (`false`) when requested to <<receiveAndReply, handle one synchronously>>

Default: `false`

Turned on (`true`) when requested to <<StopContinuousExecutionWrites, handle a synchronous StopContinuousExecutionWrites message>>
|===

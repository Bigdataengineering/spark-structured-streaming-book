== [[HDFSMetadataLog]] HDFSMetadataLog -- MetadataLog with Hadoop HDFS for Reliable Storage

`HDFSMetadataLog` is a concrete <<spark-sql-streaming-MetadataLog.adoc#, MetadataLog>> that uses Hadoop HDFS for a reliable storage.

[[formats]]
`HDFSMetadataLog` uses http://json4s.org/[Json4s] with the https://github.com/FasterXML/jackson-databind[Jackson] binding for JSON parsing (serialization and deserialization).

`HDFSMetadataLog` is further customized by the <<extensions, extensions>>.

[[extensions]]
.HDFSMetadataLogs
[cols="30,70",options="header",width="100%"]
|===
| HDFSMetadataLog
| Description

| <<spark-sql-streaming-CommitLog.adoc#, CommitLog>>
| [[CommitLog]] <<spark-sql-streaming-StreamExecution.adoc#commitLog, Offset commit log>> of <<spark-sql-streaming-StreamExecution.adoc#, streaming query execution engines>>

| <<spark-sql-streaming-CompactibleFileStreamLog.adoc#, CompactibleFileStreamLog>>
| [[CompactibleFileStreamLog]] Compactible metadata logs

| <<spark-sql-streaming-OffsetSeqLog.adoc#, OffsetSeqLog>>
| [[OffsetSeqLog]]

|===

[[creating-instance]]
`HDFSMetadataLog` takes the following to be created:

* [[sparkSession]] `SparkSession`
* [[path]] Path of the metadata log directory

While being <<creating-instance, created>> `HDFSMetadataLog` creates the <<path, path>> unless exists already.

=== [[serialize]] Writing Metadata in Serialized Format -- `serialize` Method

[source, scala]
----
serialize(metadata: T, out: OutputStream): Unit
----

`serialize`...FIXME

NOTE: `serialize` is used exclusively when `HDFSMetadataLog` is requested to <<writeBatchToFile, writeBatchToFile>> (when requested to <<add, store metadata for a batch>>).

=== [[deserialize]] Deserializing Metadata -- `deserialize` Method

[source, scala]
----
deserialize(in: InputStream): T
----

`deserialize` deserializes a metadata (of type `T`) from a given `InputStream`.

NOTE: `deserialize` is used exclusively when `HDFSMetadataLog` is requested to <<get, retrieve metadata for a batch>>.

=== [[createFileManager]] `createFileManager` Internal Method

[source, scala]
----
createFileManager(): FileManager
----

CAUTION: FIXME

NOTE: `createFileManager` is used exclusively when `HDFSMetadataLog` is <<creating-instance, created>> (and the internal <<fileManager, FileManager>> is created alongside).

=== [[get]][[get-batchId]] Retrieving Metadata By Batch Id -- `get` Method

[source, scala]
----
get(batchId: Long): Option[T]
----

NOTE: `get` is part of the <<spark-sql-streaming-MetadataLog.adoc#get, MetadataLog Contract>> to...FIXME.

`get`...FIXME

=== [[get-range]] Retrieving Metadata For Batch Id Range -- `get` Method

[source, scala]
----
get(
  startId: Option[Long],
  endId: Option[Long]): Array[(Long, T)]
----

NOTE: `get` is part of the <<spark-sql-streaming-MetadataLog.adoc#get, MetadataLog Contract>> to...FIXME.

`get`...FIXME

=== [[add]] Persisting (Storing) Metadata for Batch -- `add` Method

[source, scala]
----
add(batchId: Long, metadata: T): Boolean
----

NOTE: `add` is part of the <<spark-sql-streaming-MetadataLog.adoc#add, MetadataLog Contract>> to...FIXME.

`add` <<batchIdToPath, batchIdToPath>> for the given `batchId` and <<writeBatchToFile, writeBatchToFile>> unless <<get, already available (persisted)>>.

`add` returns `true` when the <<batchIdToPath, batchIdToPath>> for the given `batchId` was not available.

=== [[getLatest]] Retrieving Latest Committed Batch Id with Metadata If Available -- `getLatest` Method

[source, scala]
----
getLatest(): Option[(Long, T)]
----

NOTE: `getLatest` is a part of link:spark-sql-streaming-MetadataLog.adoc#getLatest[MetadataLog Contract] to retrieve the recently-committed batch id and the corresponding metadata if available in the metadata storage.

`getLatest` requests the internal <<fileManager, FileManager>> for the files in <<metadataPath, metadata directory>> that match <<batchFilesFilter, batch file filter>>.

`getLatest` takes the batch ids (the batch files correspond to) and sorts the ids in reverse order.

`getLatest` gives the first batch id with the metadata which <<get, could be found in the metadata storage>>.

NOTE: It is possible that the batch id could be in the metadata storage, but not available for retrieval.

=== [[purge]] Removing Log Entries (Purging) -- `purge` Method

[source, scala]
----
purge(thresholdBatchId: Long): Unit
----

NOTE: `purge` is part of the <<spark-sql-streaming-MetadataLog.adoc#purge, MetadataLog Contract>> to...FIXME.

`purge`...FIXME

=== [[getOrderedBatchFiles]] `getOrderedBatchFiles` Method

[source, scala]
----
getOrderedBatchFiles(): Array[FileStatus]
----

`getOrderedBatchFiles`...FIXME

NOTE: `getOrderedBatchFiles` is used when...FIXME

=== [[purgeAfter]] `purgeAfter` Method

[source, scala]
----
purgeAfter(thresholdBatchId: Long): Unit
----

`purgeAfter`...FIXME

NOTE: `purgeAfter` is used when...FIXME

=== [[batchIdToPath]] `batchIdToPath` Method

[source, scala]
----
batchIdToPath(batchId: Long): Path
----

`batchIdToPath`...FIXME

NOTE: `batchIdToPath` is used when...FIXME

=== [[isBatchFile]] `isBatchFile` Method

[source, scala]
----
isBatchFile(path: Path): Boolean
----

`isBatchFile`...FIXME

NOTE: `isBatchFile` is used when...FIXME

=== [[parseVersion]] Retrieving Version (From Text Line) -- `parseVersion` Internal Method

[source, scala]
----
parseVersion(text: String, maxSupportedVersion: Int): Int
----

`parseVersion`...FIXME

NOTE: `parseVersion` is used when...FIXME

=== [[pathToBatchId]] `pathToBatchId` Method

[source, scala]
----
pathToBatchId(path: Path): Long
----

`pathToBatchId`...FIXME

NOTE: `pathToBatchId` is used when...FIXME

=== [[writeBatchToFile]] `writeBatchToFile` Internal Method

[source, scala]
----
writeBatchToFile(metadata: T, path: Path): Unit
----

`writeBatchToFile`...FIXME

NOTE: `writeBatchToFile` is used exclusively when `HDFSMetadataLog` is requested to <<add, store metadata for a batch>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| batchFilesFilter
a| [[batchFilesFilter]] Hadoop HDFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/PathFilter.html[PathFilter] of <<isBatchFile, batch files>> (with names being long numbers)

Used when:

* `CompactibleFileStreamLog` is requested for the <<spark-sql-streaming-CompactibleFileStreamLog.adoc#compactInterval, compactInterval>>

* `HDFSMetadataLog` is requested to <<get, get batch metadata>>, <<getLatest, getLatest>>, <<getOrderedBatchFiles, getOrderedBatchFiles>>, <<purge, purge>>, and <<purgeAfter, purgeAfter>>

| fileManager
a| [[fileManager]] `CheckpointFileManager`

Used when...FIXME

| metadataPath
a| [[metadataPath]] The path to metadata directory

Used when...FIXME

|===

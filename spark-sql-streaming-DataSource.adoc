== [[DataSource]] DataSource -- Pluggable Data Source

`DataSource` is...FIXME

`DataSource` is <<creating-instance, created>> when...FIXME

TIP: Read https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-datasource.html[DataSource &mdash; Pluggable Data Sources] (for Spark SQL's batch structured queries).

[[internal-registries]]
.DataSource's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,3",options="header",width="100%"]
|===
| Name
| Description

| providingClass
a| [[providingClass]] https://docs.oracle.com/javase/8/docs/api/java/lang/Class.html[java.lang.Class] that corresponds to the <<className, className>> (that can be a fully-qualified class name or an alias of the data source)

| sourceInfo
a| [[sourceInfo]] `SourceInfo` with the name, the schema, and optional partitioning columns of a source.

Used when:

* `DataSource` <<createSource, creates a FileStreamSource>> (that requires the schema and the optional partitioning columns)

* `StreamingRelation` is link:spark-sql-streaming-StreamingRelation.adoc#apply[created] (for a `DataSource`)

|===

=== [[sourceSchema]] Describing Name and Schema of Streaming Source -- `sourceSchema` Internal Method

[source, scala]
----
sourceSchema(): SourceInfo
----

`sourceSchema`...FIXME

NOTE: `sourceSchema` is used exclusively when `DataSource` is requested for the <<sourceInfo, SourceInfo>>.

=== [[creating-instance]] Creating DataSource Instance

`DataSource` takes the following when created:

* [[sparkSession]] `SparkSession`
* [[className]] `className`, i.e. the fully-qualified class name or an alias of the data source
* [[paths]] Paths (default: `Nil`, i.e. an empty collection)
* [[userSpecifiedSchema]] Optional user-defined schema (default: `None`)
* [[partitionColumns]] Names of the partition columns (default: empty)
* [[bucketSpec]] Optional `BucketSpec` (default: `None`)
* [[options]] Configuration options (default: empty)
* [[catalogTable]] Optional `CatalogTable` (default: `None`)

`DataSource` initializes the <<internal-registries, internal registries and counters>>.

=== [[createSource]] Creating Streaming Source -- `createSource` Method

[source, scala]
----
createSource(metadataPath: String): Source
----

`createSource`...FIXME

NOTE: `createSource` is used exclusively when `MicroBatchExecution` is requested to <<spark-sql-streaming-MicroBatchExecution.adoc#logicalPlan, initialize the analyzed logical plan>>.

=== [[createSink]] Creating Streaming Sink -- `createSink` Method

[source, scala]
----
createSink(outputMode: OutputMode): Sink
----

`createSink` creates a <<spark-sql-streaming-Sink.adoc#, streaming sink>> for <<spark-sql-streaming-StreamSinkProvider.adoc#, StreamSinkProvider>> or `FileFormat` data sources.

TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-FileFormat.html[FileFormat Data Source] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book.

Internally, `createSink` creates a new instance of the <<providingClass, providingClass>> and branches off per type:

* For a <<spark-sql-streaming-StreamSinkProvider.adoc#, StreamSinkProvider>>, `createSink` simply delegates the call and requests it to <<spark-sql-streaming-StreamSinkProvider.adoc#createSink, create a streaming sink>>

* For a `FileFormat`, `createSink` creates a <<spark-sql-streaming-FileStreamSink.adoc#, FileStreamSink>> when `path` option is specified and the output mode is <<spark-sql-streaming-OutputMode.adoc#Append, Append>>

`createSink` throws a `IllegalArgumentException` when `path` option is not specified for a `FileFormat` data source:

```
'path' is not specified
```

`createSink` throws an `AnalysisException` when the given <<spark-sql-streaming-OutputMode.adoc#, OutputMode>> is different from <<spark-sql-streaming-OutputMode.adoc#Append, Append>> for a `FileFormat` data source:

```
Data source [className] does not support [outputMode] output mode
```

`createSink` throws an `UnsupportedOperationException` for unsupported data source formats:

```
Data source [className] does not support streamed writing
```

NOTE: `createSink` is used exclusively when `DataStreamWriter` is requested to <<spark-sql-streaming-DataStreamWriter.adoc#start, create and start a streaming query>>.

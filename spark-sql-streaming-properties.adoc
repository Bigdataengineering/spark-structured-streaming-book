== Configuration Properties

The following list are the properties that you can use to fine-tune Spark Structured Streaming applications.

You can set them in a link:spark-sql-SparkSession.adoc[SparkSession] upon instantiation using link:spark-sql-sparksession-builder.adoc#config[config] method.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .master("local[*]")
  .appName("My Spark Application")
  .config("spark.sql.streaming.metricsEnabled", true)
  .getOrCreate
----

[[properties]]
.Structured Streaming's Properties
[cols="1,2",options="header",width="100%"]
|===
| Name / Default Value
| Description

| `spark.sql.streaming.aggregation.stateFormatVersion`

`2`
| [[spark.sql.streaming.aggregation.stateFormatVersion]] *(internal)* State format version used by streaming aggregation operations in a streaming query.

Supported values: `1` or `2`

State between versions are tend to be incompatible, so state format version shouldn't be modified after running.

| `spark.sql.streaming.checkpointLocation`

(empty)
| [[spark.sql.streaming.checkpointLocation]] Default checkpoint directory for storing checkpoint data for streaming queries

| `spark.sql.streaming.continuous.executorQueueSize`
a| [[spark.sql.streaming.continuous.executorQueueSize]] *(internal)* The size (measured in number of rows) of the queue used in continuous execution to buffer the results of a ContinuousDataReader.

Default: `1024`

| `spark.sql.streaming.continuous.executorPollIntervalMs`
a| [[spark.sql.streaming.continuous.executorPollIntervalMs]] *(internal)* The interval (in millis) at which continuous execution readers will poll to check whether the epoch has advanced on the driver.

Default: `100` (ms)

| `spark.sql.streaming.disabledV2MicroBatchReaders`

(empty)
a| [[spark.sql.streaming.disabledV2MicroBatchReaders]] *(internal)* A comma-separated list of fully-qualified class names of data source providers for which <<spark-sql-streaming-MicroBatchReadSupport.adoc#, MicroBatchReadSupport>> is disabled. Reads from these sources will fall back to the V1 Sources.

```
spark.sql.streaming.disabledV2MicroBatchReaders=org.apache.spark.sql.kafka010.KafkaSourceProvider
```

Use <<spark-sql-streaming-SQLConf.adoc#disabledV2StreamingMicroBatchReaders, SQLConf.disabledV2StreamingMicroBatchReaders>> to get the current value.

| `spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion`

`2`
a| [[spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion]] *(internal)* State format version used by `flatMapGroupsWithState` operation in a streaming query.

Supported values:

* `1`
* `2`

| `spark.sql.streaming.maxBatchesToRetainInMemory`

`2`
a| [[spark.sql.streaming.maxBatchesToRetainInMemory]] *(internal)* The maximum number of batches which will be retained in memory to avoid loading from files.

Maximum count of versions a State Store implementation should retain in memory.

The value adjusts a trade-off between memory usage vs cache miss:

* `2` covers both success and direct failure cases
* `1` covers only success case
* `0` or negative value disables cache to maximize memory size of executors

Used exclusively when `HDFSBackedStateStoreProvider` is requested to <<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#init, initialize>>.

| `spark.sql.streaming.metricsEnabled`

`false`

| [[spark.sql.streaming.metricsEnabled]] Flag whether Dropwizard CodaHale metrics are reported for active streaming queries

Use <<spark-sql-streaming-SQLConf.adoc#streamingMetricsEnabled, SQLConf.streamingMetricsEnabled>> to get the current value

| `spark.sql.streaming.minBatchesToRetain`

`100`
a| [[spark.sql.streaming.minBatchesToRetain]] *(internal)* The minimum number of entries to retain for failure recovery

Use <<spark-sql-streaming-SQLConf.adoc#minBatchesToRetain, SQLConf.minBatchesToRetain>> to get the current value

| `spark.sql.streaming.multipleWatermarkPolicy`

`min`
a| [[spark.sql.streaming.multipleWatermarkPolicy]] *Global watermark policy* that is the policy to calculate the global watermark value when there are multiple watermark operators in a streaming query

Default: `min`

Supported values:

* `min` - chooses the minimum watermark reported across multiple operators

* `max` - chooses the maximum across multiple operators

Cannot be changed between query restarts from the same checkpoint location.

| `spark.sql.streaming.noDataProgressEventInterval`

`10000L`
a| [[spark.sql.streaming.noDataProgressEventInterval]] *(internal)* How long to wait between two progress events when there is no data (in millis) when `ProgressReporter` is requested to <<spark-sql-streaming-ProgressReporter.adoc#finishTrigger, finish a trigger>>

Use <<spark-sql-streaming-SQLConf.adoc#streamingNoDataProgressEventInterval, SQLConf.streamingNoDataProgressEventInterval>> to get the current value

| `spark.sql.streaming.numRecentProgressUpdates`

`100`
| [[spark.sql.streaming.numRecentProgressUpdates]] Number of link:spark-sql-streaming-ProgressReporter.adoc#updateProgress[progress updates to retain] for a streaming query

| `spark.sql.streaming.pollingDelay`

`10`
a| [[spark.sql.streaming.pollingDelay]] *(internal)* Time delay (in ms) before `StreamExecution` link:spark-sql-streaming-MicroBatchExecution.adoc#runBatches-batchRunner-no-data[polls for new data when no data was available in a batch].

| `spark.sql.streaming.stateStore.maintenanceInterval`

`60s`
| [[spark.sql.streaming.stateStore.maintenanceInterval]] The initial delay and how often to execute StateStore's link:spark-sql-streaming-StateStore.adoc#MaintenanceTask[maintenance task].

| `spark.sql.streaming.stateStore.providerClass`

<<spark-sql-streaming-HDFSBackedStateStoreProvider.adoc#, org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider>>
| [[spark.sql.streaming.stateStore.providerClass]] *(internal)* The fully-qualified class name of the <<spark-sql-streaming-StateStoreProvider.adoc#, StateStoreProvider>> implementation that manages state data in stateful streaming queries. This class must have a zero-arg constructor.

Use <<spark-sql-streaming-SQLConf.adoc#stateStoreProviderClass, SQLConf.stateStoreProviderClass>> to get the current value.

| `spark.sql.streaming.unsupportedOperationCheck`

`true`
| [[spark.sql.streaming.unsupportedOperationCheck]] *(internal)* When enabled (`true`), `StreamingQueryManager` link:spark-sql-streaming-UnsupportedOperationChecker.adoc#checkForStreaming[makes sure that the logical plan of a streaming query uses supported operations only].
|===

== [[EventTimeWatermarkExec]] EventTimeWatermarkExec Unary Physical Operator -- Monitoring Event-Time Watermark

`EventTimeWatermarkExec` is a unary physical operator that simply <<doExecute, updates>> the <<eventTimeStats, EventTimeStatsAccum>> internal accumulator with the values of the <<eventTime, event-time watermark column>>.

[NOTE]
====
A unary physical operator (`UnaryExecNode`) is a physical operator with a single <<child, child>> physical operator.

Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book.
====

`EventTimeWatermarkExec` uses <<eventTimeStats, EventTimeStatsAccum>> internal accumulator as a way to send the statistics (the maximum, minimum, average and update count) of the values in the <<eventTime, event-time watermark column>> that is used in:

* `ProgressReporter` for link:spark-sql-streaming-ProgressReporter.adoc#extractExecutionStats[creating execution statistics] for the most recent query execution (for you to monitor `max`, `min`, `avg`, and `watermark` event-time watermark statistics)

* `StreamExecution` to observe and possibly update event-time watermark when <<spark-sql-streaming-MicroBatchExecution.adoc#constructNextBatch-hasNewData-true, constructing the next streaming batch>>.

`EventTimeWatermarkExec` is <<creating-instance, created>> for every <<spark-sql-streaming-EventTimeWatermark.adoc#, EventTimeWatermark>> logical operator in a logical query plan when <<spark-sql-streaming-StatefulAggregationStrategy.adoc#, StatefulAggregationStrategy>> execution planning strategy is requested to plan a logical plan for execution.

NOTE: <<spark-sql-streaming-EventTimeWatermark.adoc#, EventTimeWatermark>> logical operator is created as the result of <<spark-sql-streaming-Dataset-operators.adoc#withWatermark, Dataset.withWatermark>> operator.

[[creating-instance]]
`EventTimeWatermarkExec` takes the following to be created:

* [[eventTime]] Event-time watermark column
* [[delay]] Delay interval (`CalendarInterval`)
* [[child]] Child physical operator (`SparkPlan`)

While <<creating-instance, created>>, `EventTimeWatermarkExec` registers the <<eventTimeStats, EventTimeStatsAccum>> internal accumulator (with the current `SparkContext`).

[source, scala]
----
val rates = spark
  .readStream
  .format("rate")
  .load
  .withWatermark(eventTime = "timestamp", delayThreshold = "10 seconds") // <-- creates EventTimeWatermark logical operator

// EventTimeWatermark logical operator is planned as EventTimeWatermarkExec physical operator
// Note that as a physical operator EventTimeWatermarkExec shows itself without the Exec suffix
scala> rates.explain
== Physical Plan ==
EventTimeWatermark timestamp#0: timestamp, interval 10 seconds
+- StreamingRelation rate, [timestamp#0, value#1L]

// Let's build a complete streaming pipeline
// Connecting the input (source) to the output (sink)
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val queryName = "rates-to-console"
val sq = rates
  .writeStream
  .format("console")
  .option("numRows", 1) // just the last row with the timestamp
  .option("truncate", false)
  .trigger(Trigger.ProcessingTime(15.seconds))
  .queryName(queryName)
  .start

// Let's access the underlying stream execution engine
// It should be MicroBatchExecution since we use ProcessingTime, shouldn't it?
import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper
val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery
scala> :type se
org.apache.spark.sql.execution.streaming.StreamExecution

import org.apache.spark.sql.execution.streaming.MicroBatchExecution
val engine = se.asInstanceOf[MicroBatchExecution]
scala> :type engine
org.apache.spark.sql.execution.streaming.MicroBatchExecution

val lastMicroBatch = engine.lastExecution
scala> :type lastMicroBatch
org.apache.spark.sql.execution.streaming.IncrementalExecution

// Access executedPlan that is the optimized physical query plan ready for execution
// All streaming optimizations have been applied at this point
// We just need the EventTimeWatermarkExec physical operator
val plan = lastMicroBatch.executedPlan
scala> :type plan
org.apache.spark.sql.execution.SparkPlan

// Let's find the one and only EventTimeWatermarkExec operator in the plan
import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec
val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head

// Among the properties are delay interval (in millis)
// We used withWatermark(..., delayThreshold = "10 seconds")
assert(watermarkOp.delayMs == 10000)

// Let's check out the event-time watermark stats
// They correspond to the concrete EventTimeWatermarkExec operator for a micro-batch
val stats = watermarkOp.eventTimeStats.value
scala> :type stats
org.apache.spark.sql.execution.streaming.EventTimeStats

scala> println(stats)
EventTimeStats(1560159493350,1560159479350,1.56015948635E12,15)

// Let's compare it to the latest lastExecution
val lastMicroBatch = engine.lastExecution
val plan = lastMicroBatch.executedPlan
import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec
val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head
val stats = watermarkOp.eventTimeStats.value
scala> println(stats)
EventTimeStats(1560160063350,1560160049350,1.5601600563500002E12,15)

// The stats are indeed different (on the second and third places)
----

=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is part of `SparkPlan` Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. `RDD[InternalRow]`).

Internally, `doExecute` executes <<child, child>> physical operator and maps over the partitions (using `RDD.mapPartitions`) that does the following:

1. Creates an unsafe projection for <<eventTime, eventTime>> in the output schema of <<child, child>> physical operator.

1. For every row (as `InternalRow`)

* Adds <<eventTime, eventTime>> to <<eventTimeStats, eventTimeStats>> accumulator

=== [[output]] Output Schema -- `output` Property

[source, scala]
----
output: Seq[Attribute]
----

NOTE: `output` is part of the `QueryPlan` Contract to describe the attributes of the output (aka _schema_).

`output`...FIXME

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| delayMs
a| [[delayMs]] <<delay, delay>> interval in milliseconds

Used when:

* `EventTimeWatermarkExec` is requested for the <<output, output schema>>
* `WatermarkTracker` is requested to <<spark-sql-streaming-WatermarkTracker.adoc#updateWatermark, update the event-time watermark>>

| eventTimeStats
a| [[eventTimeStats]] <<spark-sql-streaming-EventTimeStatsAccum.adoc#, EventTimeStatsAccum>> accumulator to accumulate <<eventTime, eventTime>> values from every row in a streaming batch (when `EventTimeWatermarkExec` <<doExecute, is executed>>).

NOTE: `EventTimeStatsAccum` is a Spark accumulator of `EventTimeStats` from `Longs` (i.e. `AccumulatorV2[Long, EventTimeStats]`).

NOTE: Every Spark accumulator has to be registered before use, and `eventTimeStats` is registered when `EventTimeWatermarkExec` <<creating-instance, is created>>.

|===

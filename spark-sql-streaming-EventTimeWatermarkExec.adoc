== [[EventTimeWatermarkExec]] EventTimeWatermarkExec Unary Physical Operator -- Streaming Watermark

`EventTimeWatermarkExec` is a unary physical operator that represents <<spark-sql-streaming-EventTimeWatermark.adoc#, EventTimeWatermark>> logical operator at execution time.

[NOTE]
====
A unary physical operator (`UnaryExecNode`) is a physical operator with a single <<child, child>> physical operator.

Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book.
====

When <<doExecute, executed>>, `EventTimeWatermarkExec` monitors the values of the <<eventTime, event-time watermark column>> and updates the <<eventTimeStats, EventTimeStatsAccum>> internal accumulator.

`EventTimeWatermarkExec` uses <<eventTimeStats, EventTimeStatsAccum>> internal accumulator as a way to send the statistics (the maximum, minimum, average and update count) of the values in the <<eventTime, event-time watermark column>> that is used in:

* `ProgressReporter` for link:spark-sql-streaming-ProgressReporter.adoc#extractExecutionStats[creating execution statistics] for the most recent query execution (for you to monitor `max`, `min`, `avg`, and `watermark` event-time watermark statistics)

* `StreamExecution` to observe and possibly update event-time watermark when <<spark-sql-streaming-MicroBatchExecution.adoc#constructNextBatch-hasNewData-true, constructing the next streaming batch>>.

`EventTimeWatermarkExec` is <<creating-instance, created>> exclusively when <<spark-sql-streaming-StatefulAggregationStrategy.adoc#, StatefulAggregationStrategy>> execution planning strategy is requested to plan a logical plan with <<spark-sql-streaming-EventTimeWatermark.adoc#, EventTimeWatermark>> logical operators for execution.

[[creating-instance]]
`EventTimeWatermarkExec` takes the following to be created:

* [[eventTime]] *Event time column* - the column with the (event) time for event-time watermark
* [[delay]] Delay interval (`CalendarInterval`)
* [[child]] Child physical operator (`SparkPlan`)

While <<creating-instance, created>>, `EventTimeWatermarkExec` registers the <<eventTimeStats, EventTimeStatsAccum>> internal accumulator (with the current `SparkContext`).

TIP: Check out <<spark-sql-streaming-demo-watermark-aggregation-append.adoc#, Demo: Streaming Watermark with Aggregation in Append Output Mode>> to deep dive into the internals of <<spark-sql-streaming-watermark.adoc#, Streaming Watermark>>.

=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is part of `SparkPlan` Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. `RDD[InternalRow]`).

Internally, `doExecute` executes the <<child, child>> physical operator and maps over the partitions (using `RDD.mapPartitions`).

`doExecute` creates an unsafe projection (one per partition) for the <<eventTime, column with the event time>> in the output schema of the <<child, child>> physical operator. The unsafe projection is to extract event times from the (stream of) internal rows of the child physical operator.

For every row (`InternalRow`) per partition, `doExecute` requests the <<eventTimeStats, eventTimeStats>> accumulator to <<spark-sql-streaming-EventTimeStatsAccum.adoc#add, add the event time>>.

NOTE: The event time value is in seconds (not millis as the value is divided by `1000` ).

=== [[output]] Output Schema -- `output` Property

[source, scala]
----
output: Seq[Attribute]
----

NOTE: `output` is part of the `QueryPlan` Contract to describe the attributes of the output (aka _schema_).

`output`...FIXME

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| delayMs
a| [[delayMs]] <<delay, delay>> interval in milliseconds

Used when:

* `EventTimeWatermarkExec` is requested for the <<output, output schema>>
* `WatermarkTracker` is requested to <<spark-sql-streaming-WatermarkTracker.adoc#updateWatermark, update the event-time watermark>>

| eventTimeStats
a| [[eventTimeStats]] <<spark-sql-streaming-EventTimeStatsAccum.adoc#, EventTimeStatsAccum>> accumulator to accumulate <<eventTime, eventTime>> values from every row in a streaming batch (when `EventTimeWatermarkExec` <<doExecute, is executed>>).

NOTE: `EventTimeStatsAccum` is a Spark accumulator of `EventTimeStats` from `Longs` (i.e. `AccumulatorV2[Long, EventTimeStats]`).

NOTE: Every Spark accumulator has to be registered before use, and `eventTimeStats` is registered when `EventTimeWatermarkExec` <<creating-instance, is created>>.

|===
